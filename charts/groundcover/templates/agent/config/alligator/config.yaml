{{ if .Values.agent.enabled }}
apiVersion: v1
data:
  config.yaml: |-
    startuptimeout: 120s
    concurrency: 20
    productchansize: 1000
    tracy:
      address: "ipc:///comm/tracy.data"
      timeout: 120s
      interval: 5s

    httphandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: true
      interval: 30s

    redishandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: true
      interval: 30s  

    sqlhandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: true
      interval: 30s

    kafkahandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: true
      interval: 30s

    dnshandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: true
      interval: 30s

    connStatsHandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: false
      interval: 30s

    tcpStatsHandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: false
      interval: 30s

    containerInfoHandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: true
      interval: 30s

    containerStateHandler:
      samplesthreshold: 10
      issuesthreshold: 10
      shouldSample: false
      interval: 30s
    
    nodeinfohandler:
      samplesthreshold: 20
      issuesthreshold: 20
      shouldsample: false
      interval: 1m0s

    containerrestarthandler:
      samplesthreshold: 20
      issuesthreshold: 20
      shouldsample: true
      interval: 30s

    pvcinfohandler:
      samplesthreshold: 20
      issuesthreshold: 20
      shouldsample: false
      interval: 1m0s

    pidcache:
      expiration: 5m
      purge: 20s

    cluster:
      maxhosts: 10000
      maxchildrens: 40

    storage:
      timeout: 5s

    timeseriesstorage:
      url: "{{ template "vmagent.url" . }}"
      tlsskipverify: false

    loglevel: 4 # INFO

    isk8s: true
    hostmodeconfigpath: "/config/instance_metadata.yaml"

    allowedportspercontainer: 20
    kubeletmetricsfetchinterval: 15s

    globallimiter:
      maxqueue: 10
      ageout: 100ms

    obfuscateData: {{ .Values.agent.alligator.obfuscateData }}
    dataRetention: {{ .Values.agent.alligator.dataRetention }}
    watchOnlyLocalNode: {{ .Values.agent.alligator.watchOnlyLocalNode }}
    floraEnabled: {{ .Values.agent.experimental }}
    runningNamespace: ""
    shouldDropRunningNamespaces: {{ .Values.shouldDropRunningNamespaces }}
    filteredNamespaces: {{ toYaml .Values.filteredNamespaces | nindent 6 }}
    nodelabels: {{ toYaml .Values.agent.alligator.nodelabels | nindent 6 }}
    contentTypesToDrop: {{ toYaml .Values.agent.alligator.contentTypesToDrop | nindent 6 }}

    promscalestorage:
      url: "{{ template "promscale.otelURL" . }}"
      tls: true
      tableWriterBatchMaxSize: 500
      tableWriterBatchInterval: 1s
      tableWriterBatchWriteTimeout: 5s

    clusterId: {{ .Values.clusterId }}
    region: {{ .Values.region }}

    flora: 
      networkTracer:
        enabled: true
        processingInterval: 100ms
        eBPFStatsReadingInterval: 5s
        tsConverterSyncInterval: 10s
        connectionManager:
          statusCheckIntervalTicks: 20
          connection:
            closeGracePeriod: 1s
            timeoutPeriod: 600s
            matchingTimeLimit: 5s
            matchingTrafficLimit: 51200 # 50k
            continuousProcessingErrorsLimit: 5
            totalProcessingErrorsLimit: 10
            protocols:
              http:
                bodyStoreSizeLimit: 1048576 # 1mb
                headersParseLimit: 4096 # 4kb
                chunkedTransferSizeDigitLimit: 10
        ringBuffer:
          pollingBufferSize: 10485760 # 10mb
      metrics:
        prometheusServer: 
          enabled: false
          port: ":5555"
          endpoint: "/metrics"
    k8sLogs:
      scraper:
        positionsFile: "/var/run/positions/positions.yaml"
        positionsSyncInterval: 10s
        targetSyncInterval: 10s
      clients:
      - name: "logs_client"
        enabled: true
        urlText: {{ template "loki.url" . }}/loki/api/v1/push
        useApiKey: false
        batchwait: 5s
        batchsize: 1048576
        client:
          tlsconfig:
            insecureSkipVerify: true
        backoffConfig:
          minBackoff: 500ms
          maxBackoff: 5m
          maxRetries: 10
        externalLabels:
          labelSet:
            cluster_id: {{ .Values.clusterId }}
            job: alligator
        timeout: 10s
        useRingBuffer: true
        filterRunningNamespace: false
        groundcoverTimeTarvel: -20s
      - name: "monitoring_client"
        enabled: {{ .Values.logging.enabled }}
        urlText: https://{{ .Values.logging.host }}/loki/api/v1/push
        useApiKey: true
        batchwait: 5s
        batchsize: 1048576
        client:
          tlsconfig:
            insecureSkipVerify: {{ .Values.saas.tls_skip_verify }}
        backoffConfig:
          minBackoff: 500ms
          maxBackoff: 5m
          maxRetries: 10
        externalLabels:
          labelSet:
            cluster_id: {{ .Values.clusterId }}
            job: alligator
            region: {{ .Values.region }}
            version: {{ default .Chart.AppVersion .Values.global.origin.tag }}
        timeout: 10s   
        useRingBuffer: false
        filterRunningNamespace: true
kind: ConfigMap
metadata:
  labels:
    {{ with .Values.global.groundcoverLabels }} 
    {{- toYaml . | nindent 4 }}
    {{- end }}
    {{- if .Values.agent.additionalLabels }}
{{ toYaml .Values.agent.additionalLabels | indent 4 }}
    {{- end }}
  annotations:
    groundcover_version: {{ default .Chart.AppVersion .Values.global.origin.tag }}
    {{- if .Values.agent.additionalAnnotations }}
{{ toYaml .Values.agent.additionalAnnotations | indent 4 }}
    {{- end }}
  name: alligator-configuration
  namespace: {{ .Release.Namespace }}
{{ end }}
