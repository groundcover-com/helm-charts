global:
  agent:
    enabled: true
  backend:
    enabled: true
    name:
  workflows:
    enabled: true
  vector:
    enabled: true
    tracesAsLogs:
      otlp:
        overrideHttpURL:
    logs:
      otlp:
        overrideHttpURL:
    custom:
      otlp:
        overrideHttpURL:
    health:
      overrideHttpURL:

  ingress:
    # set ingress site for inCloud / on-prem sensor only deployments
    site:

  ingestion:
    tls_skip_verify: true

  groundcover_token:
  # groundcover_token is preceding groundcoverPredefinedTokenSecret, make sure its empty if using existing secret
  # if the secret is preloaded in the namespace, you can refer it here instead
  # example for a preloaded secret :
  # apiVersion: v1
  # kind: Secret
  # metadata:
  #   name: <secretName>
  # stringData:
  #   <secretKey>: <apikey>
  # type: Opaque
  groundcoverPredefinedTokenSecret:
    # the name of the secret
    secretName:
    # the key in the secret containing the token value
    secretKey:

  origin:
    registry: public.ecr.aws/groundcovercom
    tag: "" # rewrites Chart.AppVersion
  imagePullSecrets: []

  groundcoverPartOf: groundcover
  groundcoverLabels:

  clickhouse:
    auth:
      # override to use exisiting secret
      existingSecret: ""
      existingSecretKey: admin-password

  logs:
    overrideUrl: ""
    retention: 3d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  metrics:
    tls:
      enabled: false
    overrideUrl: ""

  otlp:
    tls:
      enabled: false
    overrideGrpcURL: ""
    overrideHttpURL: ""

  datadogapm:
    overrideUrl: ""

  traces:
    retention: 24h # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  events:
    retention: 7d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  measurements:
    retention: 7d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  sources:
    retention: 30d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  entities:
    retention: 5d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  metricsMetadata:
    retention: 30d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  telemetry:
    enabled: true
    logs:
      url: https://logs.groundcover.com/v1/logs
    metrics:
      url: https://metrics.groundcover.com/api/v1/write
    traces:
      otlpUrl: https://traces.groundcover.com/v1/traces
      zipkinUrl: https://traces.groundcover.com/api/v2/spans

  monitors:
    evaluation:
      retention: 30d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    instance:
      retention: 90d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  postgresql:
    enabled: true
    overrideUrl: ""

tags:
  # InCloud - Enterprise Setup. groundcover's control-plane reconciled observability infrastructure deployment.
  incloud: false

# SAAS parameters, token is mendatory, others only if in cloud mode
saas:
  tls_skip_verify: false
  scheme: wss
  host: client.groundcover.com
  port: 443
  basePath: ""

#GENERAL
env:
clusterId:
installationId:
# multipleClusterIds:
#  - clusterA
#  - clusterB
region:
dropRunningNamespaceLogs: true
tracesNamespaceFilters:
  []
  # - matchType: "allow"
  #   regex: "my-namespace"
tracesWorkloadFilters:
  []
  # - matchType: "block"
  #   regex: "do-not-show-me"
logsDropFilters: []
# - '{namespace="demo-ng",workload="loadgenerator"} |~ ".*GET.*"'
logsMultiLines:
  []
  # - namespace: demo-ng             # mandatory
  #   workload: ".*"                 # mandatory
  #   container: "test"              # optional
  #   firstLineRegex: "PlaceOrder"   # mandatory
  #   maxLines: 100                  # optional, default = 1024
  #   maxWaitTime: 10s               # optional, default = 3s
decolorizeLogs: false
maxLogSize: 102400 # 100KB
maxLogContentSize: 5120 # 5KB
logBatchSendQueueWorkerCount: 3
logBatchSendQueueMaxSize: 5
oldLogDropThreshold: 60s
oldLogDropAlwaysThreshold: 60m
oldLogDelayStart: 60s
jsonFlattenMaxDepth: 6
jsonFlattenMaxAttributes: 300
shouldDropRunningNamespaces: true
commitHashKeyName:
repositoryUrlKeyName:

priorityClass:
  create: false
  name:
  value:
  preemptionPolicy:

#imagePullSecrets: []

volume-expansion:
  enabled: true
  image:
    tag: latest
    repository: "{{ .Values.global.origin.registry }}/bitnami/kubectl"

agent:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  tolerations:
  - operator: "Exists"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: eks.amazonaws.com/compute-type
            operator: NotIn
            values:
            - fargate
  nodeSelector:
  priorityClassName:
  hostNetwork: false
  monitoring:
    port: 9102
  initContainers:  
    checkIngestion:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}' 
    checkMetrics:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}' 
  flb:
    image:
      tag: 2.2.3
      repository: "{{ .Values.global.origin.registry }}/fluent/fluent-bit"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 64Mi
    env: []
    builtinEnv:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ template "groundcover.apikeySecretKey" . }}'
          name: '{{ template "groundcover.apikeySecretName" . }}' 
  sensor:
    image:
      repository: "{{ .Values.global.origin.registry }}/sensor"
      tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
    resources:
      requests:
        memory: 300Mi
        cpu: 160m
      limits:
        memory: 700Mi
        cpu: 800m
    nodelabels: []
    contentTypesToDrop: []
    hostHeadersToDrop: []
    headersForceSampling:
      gcForceSample: true
      w3cForceSample: false
      b3ForceSample: false
      datadogForceSample: false
    watchOnlyLocalNode: false
    apmIngestor:
      dataDog:
        proxyEnabled: false
        tracesPort: 8126
        samplingRatio: 1
      otel:
        proxyEnabled: false
        direct:
          enabled: true
          samplingRatio: 1
          zipkin:
            enabled: true
            port: 9411
          otlp:
            enabled: true
            grpcPort: 4317
            httpPort: 4318
          jaeger:
            enabled: true
            grpcPort: 14250
            thriftHttpPort: 14268
            thriftBinaryPort: 6832
            thriftCompactPort: 6831
    pprof:
      enabled: true
      interval: 10s
      exponent: 40
      uploaderType: pyroscope
      cpuSamplingDuration: 60s
    httphandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    grpchandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    redishandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
      obfuscateSensitiveValues: true
    sqlhandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    kafkahandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    dnshandler:
      truncationConfig:
        enabled: false
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    mongodbhandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    noPayloadsMode: false
    sensitiveHeadersObfuscationConfig:
      enabled: true
      mode: "ObfuscateSpecificValues"
      specificKeys:
        [
          "Authorization",
          "Proxy-Authorization",
          "X-Amz-Security-Token",
          "X-Amz-Credential",
          "X-Amz-Firehose-Access-Key",
        ]
    env:
    builtinEnv:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ template "groundcover.apikeySecretKey" . }}'
          name: '{{ template "groundcover.apikeySecretName" . }}' 
    httpUnlimitedEndpoints: "/api/v0.2/traces"
    http2UnlimitedEndpoints: "/api/v0.2/traces"
    childcountruleoverrides:
    - depth: 0
      maxChildren: 100
  priorityClass:
    create: true
    fullname:
    value: 1000000000
    preemptionPolicy: PreemptLowerPriority
dbManager:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  image:
    repository: "{{ .Values.global.origin.registry }}/db-manager"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  resources:
    limits:
      memory: 1Gi
    requests:
      cpu: 15m
      memory: 15Mi
  extraEnvVars:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  objectStorageQueues: {}
  cloudPricing: 
    enabled: true
    pricingFileURL: "https://groundcover-public-assets.s3.amazonaws.com/cloud_pricing_v1.csv"
    refreshInterval: 6h # {amount}[h(ours), d(ays), w(eeks), y(ears)]

k8sWatcher:
  initContainers:
    checkIngestion:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}'
    checkMetrics:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}'   
  watch:
    configmap:
      enabled: true
    secret:
      enabled: true
  image:
    repository: "{{ .Values.global.origin.registry }}/k8s-watcher"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  resources:
    limits:
      cpu: 2000m
      memory: 1024Mi
    requests:
      cpu: 500m
      memory: 256Mi
  pprof:
    enabled: true
    interval: 10s
    exponent: 40
    uploaderType: pyroscope
    cpuSamplingDuration: 60s
  env: []
  builtinEnv:
  - name: API_KEY
    valueFrom:
      secretKeyRef:
        key: '{{ template "groundcover.apikeySecretKey" . }}'
        name: '{{ template "groundcover.apikeySecretName" . }}'        
  readinessProbe:
    httpGet:
      scheme: HTTP
      path: /health
      port: watcher-http
    initialDelaySeconds: 45
    periodSeconds: 15
    failureThreshold: 20
  livenessProbe:
    httpGet:
      scheme: HTTP
      path: /health
      port: watcher-http
    initialDelaySeconds: 45
    periodSeconds: 15
    failureThreshold: 20

portal:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  env:
    - name: GC_CLICKHOUSE_HOST
      value: '{{ include "clickhouse.shard0Name" . }}'
  extraHeaders:
  # - Key: "Header Key"
  #   Value: "Header Value"
  image:
    repository: "{{ .Values.global.origin.registry }}/portal"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 50m
      memory: 256Mi
  pprof:
    enabled: true
    interval: 10s
    exponent: 40
    uploaderType: pyroscope
    cpuSamplingDuration: 60s
  autodiscovery: false
  maxResponseSizeBytes: 15728640

clickhouse:
  enabled: true
  shards: 1
  replicaCount: 1
  image:
    registry: public.ecr.aws/groundcovercom
  containerSecurityContext:
    enabled: false
  podLabels:
    app.kubernetes.io/part-of: '{{ include "groundcover.labels.partOf" . }}'
  podSecurityContext:
    enabled: false
  volumePermissions:
    enabled: true
    image:
      registry: public.ecr.aws/groundcovercom
      repository: bitnami/os-shell
      tag: 11-debian-11-r91
  keeper:
    enabled: false
  zookeeper:
    enabled: false
  externalAccess:
    enabled: true
    service:
      type: ClusterIP
      ports:
        http: 8123
  metrics:
    enabled: true
  auth:
    existingSecret: "false" # override global.clickhouse.auth.existingSecret to use exisiting secret
  extraEnvVars:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  extraOverrides: |
    <clickhouse>
      <max_table_size_to_drop>0</max_table_size_to_drop>
      <max_partition_size_to_drop>0</max_partition_size_to_drop>
      <merge_tree>
        <materialize_ttl_recalculate_only>1</materialize_ttl_recalculate_only>
        <max_suspicious_broken_parts_bytes>5368709120</max_suspicious_broken_parts_bytes>
        <max_suspicious_broken_parts>1000</max_suspicious_broken_parts>
      </merge_tree>
      <profiles>
        <default>
          <date_time_input_format>best_effort</date_time_input_format>
          <async_insert>true</async_insert>
          <max_execution_time>60</max_execution_time>
          <wait_for_async_insert>true</wait_for_async_insert>
        </default>
      </profiles>
      <query_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </query_log>
      <trace_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </trace_log>
      <metric_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </metric_log>
    </clickhouse>
  usersExtraOverrides: |
    <clickhouse>
      <profiles>
        <reader_profile>
          <readonly>0</readonly>
        </reader_profile>
        <writer_profile>
          <readonly>0</readonly>
        </writer_profile>
      </profiles>
      <users>
        <reader>
          <password from_env="CLICKHOUSE_PASSWORD" />
          <networks>
            <ip>::/0</ip>
          </networks>
          <profile>reader_profile</profile>
          <quota>default</quota>
        </reader>
        <writer>
          <password from_env="CLICKHOUSE_PASSWORD" />
          <networks>
            <ip>::/0</ip>
          </networks>
          <profile>writer_profile</profile>
          <quota>default</quota>
        </writer>
      </users>
    </clickhouse>
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.labels Persistent Volume Claim labels
    ##
    labels:
      app.kubernetes.io/managed-by: Helm
      helm.sh/chart: clickhouse-3.2.1
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 256Gi

  ## ClickHouse resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ## @param resources.limits The resources limits for the ClickHouse containers
  ## @param resources.requests The requested resources for the ClickHouse containers
  ##
  resources:
    requests:
      cpu: 300m
      memory: 2048Mi
    limits:
      memory: 4096Mi

  ## @param affinity Affinity for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `podAffinityPreset`, `podAntiAffinityPreset`, and `nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param updateStrategy.type ClickHouse statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##

  extraVolumes:
    - name: clickhouse-client-config
      configMap:
        name: clickhouse-client-config-map

  extraVolumeMounts:
    - name: clickhouse-client-config
      mountPath: /etc/clickhouse-client/

opentelemetry-collector:
  enabled: true
  mode: deployment
  extraEnvs:
    - name: GC_GROUNDCOVERVERSION
      value: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
    - name: DB_MANAGER_ADDRESS
      value: '{{ include "db-manager.ready.http.url" .}}'
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
    - name: TLS_CONFIG
      value: '{{ include "opentelemetry-collector.tlsConfig" . }}'
    - name: CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
  extraVolumes:
    - name: certificate
      secret:
        secretName: opentelemetry-collector-certificate
    - name: persistent-queues
      emptyDir: {}
  extraVolumeMounts:
    - name: certificate
      readOnly: true
      mountPath: /etc/ssl/certs
    - name: persistent-queues
      mountPath: /persistent_queues
  image:
    repository: public.ecr.aws/groundcovercom/otel/opentelemetry-collector-contrib
    tag: ""
  podLabels:
    app.kubernetes.io/part-of: '{{ include "groundcover.labels.partOf" . }}'
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: '{{ index .Values "ports" "metrics" "containerPort" | int }}'
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
  ports:
    metrics:
      enabled: true
    pprof:
      enabled: true
      containerPort: 1777
      servicePort: 1777
      hostPort: 1777
      protocol: TCP
    faro:
      enabled: true
      containerPort: 12347
      servicePort: 12347
      hostPort: 12347
      protocol: TCP
    loki-historian:
      enabled: true
      containerPort: 3500
      servicePort: 3500
      hostPort: 3500
      protocol: TCP
    loki-http:
      enabled: true
      containerPort: 3100
      servicePort: 3100
      hostPort: 3100
      protocol: TCP
    health:
      enabled: true
      containerPort: 13133
      servicePort: 13133
      hostPort: 13133
      protocol: TCP
    datadogapm:
      enabled: true
      containerPort: 8126
      servicePort: 8126
      hostPort: 8126
      protocol: TCP
    awsfirehose:
      enabled: true
      containerPort: 4433
      servicePort: 4433
      hostPort: 4433
      protocol: TCP
  config:
    extensions:
      health_check:
        path: /health
        tls: ${env:TLS_CONFIG}
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "health" "containerPort" | int ) }}'
      pprof:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "pprof" "containerPort" | int ) }}'
    receivers:
      otlp/dummy_receiver:
        protocols:
          http:
            endpoint: "localhost:9999"
      loki/historian:
        protocols:
          http:
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "loki-historian" "containerPort" | int ) }}'
        use_incoming_timestamp: true
      loki:
        protocols:
          http:
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "loki-http" "containerPort" | int ) }}'
        use_incoming_timestamp: true
      otlp:
        protocols:
          grpc:
            max_recv_msg_size_mib: 50
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "otlp" "containerPort" | int ) }}'
          http:
            tls: ${env:TLS_CONFIG}
            include_metadata: true
            cors:
              allowed_origins:
                - "*"
              allowed_headers:
                - "*"
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "otlp-http" "containerPort" | int ) }}'
      faro:
        http:
          tls: ${env:TLS_CONFIG}
          cors:
            allowed_origins:
              - "*"
            allowed_headers:
              - "*"
          endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "faro" "containerPort" | int ) }}'
      datadog:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "datadogapm" "containerPort" | int ) }}'
      awsfirehose:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "awsfirehose" "containerPort" | int ) }}'
        record_type: "cwlogs"
    processors:
      batch/monitorState:
        timeout: 5s
        send_batch_size: 1000
      batch/logs:
        timeout: 1s
        send_batch_size: 20000
      batch/custom:
        timeout: 5s
        send_batch_size: 1000
      batch/traces:
        timeout: 1s
        send_batch_size: 5000
      filter/dropNoneCustom:
        error_mode: ignore
        logs:
          log_record:
            # custom logs have explicit log_type or type so log is excluded
            - attributes["log_type"] == "log"
            - attributes["type"] == nil
      filter/dropNoneLogs:
        logs:
          log_record:
            # logs can have either explicit 'log' log_type or no log_type at all and no type (reserved for custom logs)
            - attributes["log_type"] != nil and attributes["log_type"] != "log"
            - attributes["type"] != nil
      resource/enrich_headers:
        attributes:
        - key: env_name
          from_context: x-groundcover-env-name
          action: upsert
        - key: service.name
          from_context: x-groundcover-service-name
          action: upsert
        - key: gc_env_type
          from_context: x-groundcover-env-type
          action: upsert
      groundcover/logs:
        type: logs
      groundcover/custom:
        type: custom
      groundcover/monitorState:
        type: monitorState
      groundcover/traces: {}
    exporters:
      prometheusremotewrite:
        endpoint: '{{ include "metrics-ingester.cluster.http.write.url" . }}'
        tls:
          insecure: true
        resource_to_telemetry_conversion:
          enabled: true
      groundcover/monitorState:
        type: monitorState
        timeout: 5s
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        retry_on_failure:
          initial_interval: 5s
          max_interval: 5s
          max_elapsed_time: 15s
      groundcover/logs:
        type: logs
        timeout: 5s
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        retry_on_failure:
          initial_interval: 5s
          max_interval: 5s
          max_elapsed_time: 15s
      groundcover/custom:
        type: custom
        timeout: 5s
        ttl_interval: 5d
        logs_table_name: k8s_objects
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        retry_on_failure:
          initial_interval: 5s
          max_interval: 5s
          max_elapsed_time: 15s
      groundcover/traces:
        type: traces
        timeout: 5s
        traces_table_name: traces
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        retry_on_failure:
          initial_interval: 5s
          max_interval: 5s
          max_elapsed_time: 15s
      otlphttp/vector_logs:
        endpoint: '{{ include "vector.cluster.otlp.http.logs.endpoint" . }}'
        tls:
          insecure: true
      otlphttp/vector_traces:
        endpoint: '{{ include "vector.cluster.otlp.http.traces.endpoint" . }}'
        tls:
          insecure: true
      otlphttp/vector_custom:
        endpoint: '{{ include "vector.cluster.otlp.http.custom.endpoint" . }}'
        tls:
          insecure: true
      otlphttp/vector_monitors:
        endpoint: '{{ include "vector.cluster.otlp.http.monitors.endpoint" . }}'
        tls:
          insecure: true
    connectors:
      groundcover/traces2logs: {}
    service:
      telemetry:
        metrics:
          address: '{{ printf "0.0.0.0:%d" (index .Values "ports" "metrics" "containerPort" | int ) }}'
      extensions: [health_check, pprof]
      pipelines:
        logs/monitorState:
          receivers:
            - loki/historian
          processors:
            - batch/monitorState
            - groundcover/monitorState
          exporters:
            - '{{ ternary "otlphttp/vector_monitors" "groundcover/monitorState"  .Values.global.vector.enabled }}'
        metrics:
          receivers:
            - otlp
          exporters:
            - prometheusremotewrite
        traces:
          receivers:
            - otlp
            - datadog
            - faro
          processors:
            - batch/traces
            - groundcover/traces
          exporters:
            - '{{ ternary "groundcover/traces2logs" "groundcover/traces"  .Values.global.vector.enabled }}'
        logs:
          receivers:
            - loki
            - otlp
            - faro
            - awsfirehose
          processors:
            - filter/dropNoneLogs
            - resource/enrich_headers
            - batch/logs
            - groundcover/logs
          exporters:
            - '{{ ternary "otlphttp/vector_logs" "groundcover/logs"  .Values.global.vector.enabled }}'
        logs/traces:
          receivers:
            - '{{ ternary "groundcover/traces2logs" "otlp/dummy_receiver" .Values.global.vector.enabled}}'
          exporters:
            - otlphttp/vector_traces
        logs/custom:
          receivers:
            - otlp
          processors:
            - filter/dropNoneCustom
            - batch/custom
            - groundcover/custom
          exporters:
            - '{{ ternary "otlphttp/vector_custom" "groundcover/custom"  .Values.global.vector.enabled }}'
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      memory: 2048Mi
  podDisruptionBudget:
    enabled: true
    maxUnavailable: "25%"
  rollout:
    maxUnavailable: 0
    maxSurge: 1
    strategy: RollingUpdate
  affinity: {}
  nodeSelector: {}
  tolerations: []

vector:
  rollWorkload: false
  logsPipeline:
    inputs:
      - logs_from_logs
      - json_logs
    defaultSteps:
    - name: dropOldLogs
      transform:
        type: "filter"
        condition: |
          to_unix_timestamp(now()) - 12*3600 < to_unix_timestamp(parse_timestamp!(.timestamp, "%+"))
    extraSteps: []
  # List of steps to perform as part of the pipeline. Will be executed sequentially. Examples:
  # - name: renameAttribute
  #   transform:
  #     type: remap
  #     source: |-
  #       .string_attributes.newName = del(.string_attributes.oldName)
  # - name: parseRegex
  #   transform:
  #     type: remap
  #     source: |-
  #       . |= parse_regex(.content, r'pattern')
  tracesPipeline:
    inputs:
      - traces_from_logs
      - json_traces
    defaultSteps: []
    extraSteps: []
  # List of steps to perform as part of the pipeline. Will be executed sequentially. Examples:
  # - name: renameAttribute
  #   transform:
  #     type: remap
  #     source: |-
  #       .string_attributes.newName = del(.string_attributes.oldName)
  # - name: parseRegex
  #   transform:
  #     type: remap
  #     source: |-
  #       . |= parse_regex(.content, r'pattern')
  eventsPipelines: {}
  # map of pipeline names to pipeline configurations
  # every pipeline configuration is made of a list of steps. for example:
    # random_number_event:
    #   inputs:
    #     - logs_from_logs
    #     - json_logs
    #   defaultSteps: []
    #   extraSteps:
    #   - name: filterWorkloadAndContent
    #     transform:
    #       type: "filter"
    #       condition: |
    #         .workload == "random-logger" && contains(string!(.content), "Random number")
    #   - name: parseRandomNumber
    #     transform:
    #       type: "remap"
    #       source: |-
    #         res = parse_regex!(string!(.content), r'Random number: (?P<number>[\d]+)')
    #         .float_attributes = object!(.float_attributes)
    #         .float_attributes.random_number = to_int!(res.number)
  image:
    repository: public.ecr.aws/groundcovercom/vector
  service:
    ports:
      - name: api
        port: 8686
      - name: grpc-logs
        port: 4317
      - name: http-logs
        port: 4318
      - name: json-logs
        port: 4319
      - name: grpc-traces
        port: 4327
      - name: http-traces
        port: 4328
      - name: json-traces
        port: 4329
      - name: grpc-custom
        port: 4337
      - name: http-custom
        port: 4338
      - name: json-events
        port: 4359
      - name: json-entities
        port: 4369
      - name: json-measurements
        port: 4379
      - name: grpc-monitors
        port: 4347
      - name: http-monitors
        port: 4348
      - name: json-monitors
        port: 4349
      - name: metrics
        port: 9598
      - name: vm-metrics
        port: 4599
      - name: statsd-udp
        port: 8125
        protocol: UDP
      - name: statsd-tcp
        port: 8125
  containerPorts:
    - name: api
      containerPort: 8686
    - name: grpc-logs
      containerPort: 4317
    - name: http-logs
      containerPort: 4318
    - name: json-logs
      containerPort: 4319
    - name: grpc-traces
      containerPort: 4327
    - name: http-traces
      containerPort: 4328
    - name: json-traces
      containerPort: 4329
    - name: grpc-custom
      containerPort: 4337
    - name: http-custom
      containerPort: 4338
    - name: json-custom
      containerPort: 4339
    - name: grpc-monitors
      containerPort: 4347
    - name: http-monitors
      containerPort: 4348
    - name: json-monitors
      containerPort: 4349
    - name: metrics
      containerPort: 9598
    - name: vm-metrics
      containerPort: 4599
    - name: statsd-udp
      containerPort: 8125
      protocol: UDP
    - name: statsd-tcp
      containerPort: 8125
  role: "Stateless-Aggregator"
  replicas: 2
  resources:
    requests:
      cpu: 100m
      memory: 2048Mi
    limits:
      memory: 4096Mi
  # backendEnv will only be created if backend is enabled
  backendEnv:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  env:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
    - name: VECTOR_WATCH_CONFIG
      value: "true"
  podAnnotations:
    prometheus.io/port: "9598"
    prometheus.io/scrape: "true"
  podLabels:
    app.kubernetes.io/part-of: groundcover
  existingConfigMaps:
    - vector-config-map
  dataDir: "/data/vector.yaml"
  readinessProbe:
    httpGet:
      path: /health
      port: 8686
    initialDelaySeconds: 2
    timeoutSeconds: 1
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3
  livenessProbe:
    httpGet:
      path: /health
      port: 8686
    initialDelaySeconds: 2
    timeoutSeconds: 1
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  objectStorage:
    allowed: true
    s3Bucket:
    region:
  customGlobalConfig:
    api:
      enabled: true
      playground: false
      graphql: false
      address: "0.0.0.0:8686"
  customComponents:
    sources:
      overrideSources:
      extraSources:
      empty:
        emptySource:
          type: file
          include:
            - /non_exiding_dir/non_existing_file
          exclude:
            - /*
      metrics:
        statsd_udp:
          type: statsd
          address: 0.0.0.0:8125
          mode: udp
        statsd_tcp:
          type: statsd
          address: 0.0.0.0:8125
          mode: tcp
        vm_metrics:
          type: prometheus_remote_write
          address: 0.0.0.0:4599
          mode: tcp
      otel:
        otel_logs:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4317
          http:
            address: 0.0.0.0:4318
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
        otel_logs_custom:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4337
          http:
            address: 0.0.0.0:4338
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
        otel_logs_monitors:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4347
          http:
            address: 0.0.0.0:4348
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
        otel_traces:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4327
          http:
            address: 0.0.0.0:4328
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
      json:
        json_logs:
          type: http
          address: "0.0.0.0:4319"
          decoding:
            codec: json
        json_events:
          type: http
          address: "0.0.0.0:4359"
          decoding:
            codec: json
        json_entities:
          type: http
          address: "0.0.0.0:4369"
          decoding:
            codec: json
        json_measurements:
          type: http
          address: "0.0.0.0:4379"
          decoding:
            codec: json
        json_monitors:
          type: http
          address: "0.0.0.0:4349"
          decoding:
            codec: json
        json_traces:
          type: http
          address: "0.0.0.0:4329"
          decoding:
            codec: json
    transforms:
      overrideTransforms:
      extraTransforms:
      default:
        logs_from_logs:
          inputs:
            - "otel_logs.logs"
          type: "remap"
          source: |-
            .attributes, err = object(.attributes)
            if err == null {
              if exists(.message) {
                message, err = string(.message)
                if err == null && strlen(message) > 0 {
                  .attributes.body = message
                }
              }

              if exists(.resources) {
                .resources, err = object(.resources)
                if err == null {
                  .attributes = merge(.attributes, .resources, true)
                }
              }

              . = .attributes
            } else {
              log("Recieved non-object attributes, skipping", level: "error")
            }
        traces_from_logs:
          inputs:
            - "otel_traces.logs"
          type: "remap"
          source: |-
            . = .attributes
        custom_from_logs:
          inputs:
            - "otel_logs_custom.logs"
          type: "remap"
          source: |-
            . = .attributes
        custom_from_logs_routing:
          inputs:
            - custom_from_logs
          type: "route"
          reroute_unmatched: true
          route:
            entities: .log_type == "entity"
            measurements: .log_type == "measurement"
        entities:
          inputs:
            - custom_from_logs_routing.entities
          type: "remap"
          source: |-

        measurements:
          inputs:
            - custom_from_logs_routing.measurements
          type: "remap"
          source: |-

        events:
          inputs:
            - custom_from_logs_routing._unmatched
          type: "remap"
          source: |-
            .type = del(.event_type)
        monitors:
          inputs:
            - "otel_logs_monitors.logs"
          type: "remap"
          source: |-
            . = .attributes
            del(.condition)
            del(.dashboardUID)
            del(.folderUID)
            del(.grafana_folder)
            del(.from)
            del(.group)
            del(.orgID)
            del(.panelID)
            del(.ruleUID)
            .previous_state, err = to_string(del(.previous))
            if err == null {
              .previous_state = to_string(split(.previous_state, "(", 2)[0])
              .previous_state = strip_whitespace(.previous_state)
            }
            .state, err = to_string(del(.current))
            if err == null {
              .state = to_string(split(.state, "(", 2)[0])
              .state = strip_whitespace(.state)
            }
            .cluster = get!(value: .labels, path: ["cluster"])
            if .cluster == "" {
              .cluster = get!(value: .labels, path: ["clusterId"])
            }
            .env = get!(value: .labels, path: ["env"])
            .namespace = get!(value: .labels, path: ["namespace"])
            .workload = get!(value: .labels, path: ["workload"])
            if .workload == "" {
              .workload = get!(value: .labels, path: ["workload_name"])
            } 
            .pod = get!(value: .labels, path: ["pod"])
            if .pod == "" {
              .pod = get!(value: .labels, path: ["pod_name"])
            }
            .category = get!(value: .labels, path: ["category"])
            .severity = get!(value: .labels, path: ["_gc_severity"])
        metrics_metadata_to_logs:
          inputs:
            - "vm_metrics"
          type: "metric_to_log"
        throttled_metrics_metadata_to_logs:
          inputs:
            - "metrics_metadata_to_logs"
          type: "throttle"
          threshold: 1000
          window_secs: 1
        metrics_metadata_extracted:
          inputs:
            - "throttled_metrics_metadata_to_logs"
          type: "remap"
          source: |-
            . =  {
              "timestamp": .timestamp,
              "cluster": .tags.clusterId,
              "env": .tags.env,
              "namespace": .tags.namespace,
              "name": .name,
              "tags": .tags
            }
      logs_to_events: # inputs list should be injected
        type: "remap"
        source: |-
          if !exists(.event_type) {
            abort "no event_type for custom event"
          }
          . = {
            "event_id": .guid,
            "first_timestamp": .timestamp,
            "last_timestamp": .timestamp,
            "created_timestamp": .timestamp,
            "seen_timestamp": .timestamp,
            "timestamp": .timestamp,
            "env": .env,
            "source": .source,
            "is_external": false,
            "entity_uid": .pod_uid,
            "entity_name": .pod_name,
            "entity_kind": "pod",
            "entity_namespace": .namespace,
            "entity_workload": .workload,
            "category": "custom",
            "type": .event_type,
            "severity": "",
            "k8s_event_uid": "",
            "k8s_reason": "",
            "reason": "",
            "k8s_message": "",
            "message": .content,
            "count": 1,
            "raw": .content,
            "string_attributes": .string_attributes,
            "float_attributes": .float_attributes,
            "cluster": .cluster,
            "env_name": .env,
          }

    sinks:
      overrideSinks:
      extraSinks:
      metrics:
        custom_metrics_sink:
          inputs:
            - statsd_udp
            - statsd_tcp
          type: prometheus_remote_write
          endpoint: '{{ include "metrics-ingester.cluster.http.write.url" . }}'
          healthcheck:
            enabled: false
      remote:
        logs:
          json_logs_sink:
            type: http
            uri: '{{ include "vector.incloud.json.logs.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 50000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
        traces:
          json_traces_sink:
            type: http
            uri: '{{ include "vector.incloud.json.traces-as-logs.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
        custom:
          json_events_sink:
            type: http
            inputs:
            - events
            - json_events
            - logs_to_events_transform
            uri: '{{ include "vector.incloud.json.events.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
          json_entities_sink:
            type: http
            inputs:
              - entities
              - json_entities
            uri: '{{ include "vector.incloud.json.entities.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
          json_measurements_sink:
            type: http
            inputs:
              - measurements
              - json_measurements
            uri: '{{ include "vector.incloud.json.measurements.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
      local:
        logs:
          clickhouse_logs:
            type: clickhouse
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "logs_v3"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 50000
                when_full: drop_newest
            batch:
              max_bytes: 200000000
              max_events: 200000
              timeout_secs: 1
        traces:
          clickhouse_traces:
            type: clickhouse
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "traces_v2"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 200000000
              max_events: 10000
              timeout_secs: 1
        custom:
          clickhouse_k8s_entities:
            type: clickhouse
            inputs:
              - entities
              - json_entities
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "entities"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_measurements:
            type: clickhouse
            inputs:
              - measurements
              - json_measurements
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "measurements"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_k8s_events:
            type: clickhouse
            inputs:
            - events
            - json_events
            - logs_to_events_transform
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "events_v2"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_monitors:
            type: clickhouse
            inputs:
              - monitors
              - json_monitors
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "monitor_state"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_metrics_metadata:
            type: clickhouse
            inputs:
              - metrics_metadata_extracted
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "metrics_metadata"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
      s3:
        logs:
          s3_logs:
            type: "aws_s3"
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/logs/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
        traces:
          s3_traces:
            type: "aws_s3"
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/traces/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 2500
              timeout_secs: 5
            healthcheck:
              enabled: false
        custom:
          s3_entities:
            type: "aws_s3"
            inputs:
              - entities
              - json_entities
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/entities/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
          s3_measurements:
            type: "aws_s3"
            inputs:
              - measurements
              - json_measurements
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/measurements/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
          s3_events:
            type: "aws_s3"
            inputs:
            - events
            - json_events
            - logs_to_events_transform
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/events/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
      gcs:
        logs:
          gcs_logs:
            type: gcp_cloud_storage
            key_prefix: v2/logs/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
        traces:
          gcs_traces:
            type: gcp_cloud_storage
            key_prefix: v2/traces/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 2500
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
        custom:
          gcs_entities:
            type: gcp_cloud_storage
            key_prefix: v2/entities/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
          gcs_measurements:
            type: gcp_cloud_storage
            key_prefix: v2/measurements/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
          gcs_events:
            type: gcp_cloud_storage
            key_prefix: v2/events/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false

router:
  # onprem or cloud
  mode: cloud
  enabled: false
  additionalLabels:
  additionalAnnotations:

metrics-ingester:
  nodeSelector:
  rbac:
    enabled: false
  service:
    enabled: true
  podAnnotations:
    prometheus.io/port: "8429"
    prometheus.io/scrape: "true"
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
  extraArgs:
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    remoteWrite.showURL: "true"
    remoteWrite.maxDiskUsagePerURL: '{{ include "ingester.buildRemoteWriteMaxDiskUsagePerURL" . }}'
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.tlsInsecureSkipVerify: '{{ ternary "true" "false" .Values.global.ingestion.tls_skip_verify }}'
    remoteWrite.url: '{{ include "ingester.buildRemoteWriteURLTargets" . }}'
    remoteWrite.rateLimit: '{{ include "ingester.buildRemoteWriteRateLimit" . }}'
    remoteWrite.disableOnDiskQueue: '{{ include "ingester.buildRemoteWriteDisableOnDiskQueue" . }}'
    tlsKeyFile: /etc/ssl/gc-certs/tls.key
    tlsCertFile: /etc/ssl/gc-certs/tls.crt
    tls: "{{ .Values.global.metrics.tls.enabled }}"
    usePromCompatibleNaming: "true"
  env:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  extraVolumes:
    - name: certificate
      secret:
        optional: true
        secretName: metrics-ingester-certificate
  extraVolumeMounts:
    - name: certificate
      readOnly: true
      mountPath: /etc/ssl/gc-certs
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
  rollout:
    maxUnavailable: 0
    maxSurge: 1
    strategy: RollingUpdate
  config:
    scrape_configs: []

victoria-metrics-agent:
  nodeSelector:
  rbac:
    namespaced: true
  resources:
    limits:
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  podAnnotations:
    prometheus.io/port: "8429"
    prometheus.io/scrape: "true"
  # imagePullSecrets:
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  extraArgs:
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    remoteWrite.showURL: "true"
    remoteWrite.maxDiskUsagePerURL: 10GB
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.url: '{{ include "telemetry.metrics.url" . }}'
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID),groundcover_version=$(GC_VERSION)
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
    - name: GC_VERSION
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_VERSION
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  config:
    scrape_configs:
      - job_name: groundcover
        kubernetes_sd_configs:
          - namespaces:
              own_namespace: true
            role: pod
        relabel_configs:
          - action: keep
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: ".*groundcover.*"
          - action: drop
            regex: true
            source_labels:
              - __meta_kubernetes_pod_container_init
          - action: keep_if_equal
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_port
              - __meta_kubernetes_pod_container_port_number
          - action: keep
            regex: true
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scrape
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scheme
            target_label: __scheme__
          - action: replace
            regex: (.+)
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_path
            target_label: __metrics_path__
          - action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            source_labels:
              - __address__
              - __meta_kubernetes_pod_annotation_prometheus_io_port
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - action: replace
            source_labels:
              - __meta_kubernetes_namespace
            target_label: kubernetes_namespace
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_name
            target_label: kubernetes_pod_name
          - source_labels:
              - __meta_kubernetes_pod_name
            target_label: instance
        tls_config:
          insecure_skip_verify: true

kube-state-metrics:
  nodeSelector:
  nameOverride: kube-state-metrics
  customLabels:
    app.groundcover.com/owner: groundcover
  service:
    port: 8080
    annotations:
      prometheus.io/port: "8080"
  collectors:
    # - certificatesigningrequests
    - configmaps
    - cronjobs
    - daemonsets
    - deployments
    # - endpoints
    - horizontalpodautoscalers
    # - ingresses
    - jobs
    # - leases
    # - limitranges
    # - mutatingwebhookconfigurations
    # - namespaces
    # - networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    # - poddisruptionbudgets
    - pods
    - replicasets
    - replicationcontrollers
    - resourcequotas
    # - secrets
    # - services
    - statefulsets
  # - storageclasses
  # - validatingwebhookconfigurations
  # - volumeattachments

victoria-metrics-operator:
  enabled: false
  builtinVMAgent:
    enabled: false
    spec:
      podScrapeSelector: {}
      serviceScrapeSelector: {}
      nodeScrapeSelector: {}
      staticScrapeSelector: {}
      probeSelector: {}
      replicaCount: 1
      remoteWrite:
        - url: '{{ include "custom-metrics.write.http.url" . }}'

custom-metrics:
  enabled: false
  service:
    enabled: true
  nodeSelector:
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8429"
  rbac:
    namespaced: false
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  resources:
    limits:
      memory: 1024Mi
    requests:
      cpu: 100m
      memory: 256Mi
  extraArgs:
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    promscrape.suppressDuplicateScrapeTargetErrors: "true"
    remoteWrite.showURL: "true"
    remoteWrite.maxDiskUsagePerURL: 10GB
    remoteWrite.maxDailySeries:  '{{ ternary "1000000" "5000000" (empty .Values.global.ingress.site) }}'
    remoteWrite.maxHourlySeries: '{{ ternary "200000" "1000000" (empty .Values.global.ingress.site) }}'
    promscrape.maxScrapeSize: "64MiB"
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.tlsInsecureSkipVerify: '{{ ternary "true" "false" .Values.global.ingestion.tls_skip_verify }}'
    remoteWrite.url: '{{ include "victoria-metrics.write.http.url" . }}'
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID)
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  extraScrapeConfigs: []
  config:
    scrape_configs:
      ## COPY from Prometheus helm chart https://github.com/helm/charts/blob/master/stable/prometheus/values.yaml
      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: "kubernetes-apiservers"
        honor_labels: true
        kubernetes_sd_configs:
          - role: endpoints
        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https
        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - source_labels:
              [
                __meta_kubernetes_namespace,
                __meta_kubernetes_service_name,
                __meta_kubernetes_endpoint_port_name,
              ]
            action: keep
            regex: default;kubernetes;https
      - job_name: "kubernetes-nodes"
        honor_labels: true
        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https
        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics
      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: "kubernetes-service-endpoints"
        honor_labels: true
        kubernetes_sd_configs:
          - role: endpointslices
        relabel_configs:
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_name
              - __meta_kubernetes_pod_label_app_groundcover_com_owner
            regex: kube-state-metrics;groundcover
          - action: drop
            source_labels: [__meta_kubernetes_pod_container_init]
            regex: true
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              [
                __address__,
                __meta_kubernetes_service_annotation_prometheus_io_port,
              ]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node
      # Scrape config for slow service endpoints; same as above, but with a larger
      # timeout and a larger interval
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: "kubernetes-service-endpoints-slow"
        honor_labels: true
        scrape_interval: 5m
        scrape_timeout: 30s
        kubernetes_sd_configs:
          - role: endpointslices
        relabel_configs:
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_name
              - __meta_kubernetes_pod_label_app_groundcover_com_owner
            regex: kube-state-metrics;groundcover
          - action: drop
            source_labels: [__meta_kubernetes_pod_container_init]
            regex: true
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
            action: keep
            regex: true
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              [
                __address__,
                __meta_kubernetes_service_annotation_prometheus_io_port,
              ]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node
      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: "kubernetes-services"
        honor_labels: true
        metrics_path: /probe
        params:
          module: [http_2xx]
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - source_labels:
              [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: true
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: "kubernetes-pods"
        honor_labels: true
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_name
              - __meta_kubernetes_pod_label_app_groundcover_com_owner
            regex: kube-state-metrics;groundcover
          - action: drop
            source_labels: [__meta_kubernetes_pod_container_init]
            regex: true
          - source_labels:
              [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

victoria-metrics-single:
  # imagePullSecrets: []
  server:
    image:
      repository: public.ecr.aws/groundcovercom/victoria-metrics
      tag: v1.103.0

    extraArgs:
      search.maxUniqueTimeseries: "2000000"
      search.maxSeries: "30000"
      search.maxTagKeys: "200000"
      search.maxTagValues: "500000"
      search.maxQueryLen: "100000"
      search.maxConcurrentRequests: "8"
      maxLabelsPerTimeseries: "50"
      promscrape.maxScrapeSize: "100MB"
      streamAggr.config: "/extra-config/aggregation_config.yaml"
      selfScrapeInterval: "30s"
      usePromCompatibleNaming: "true"

    # -- Data retention period, {amount}[h(ours), d(ays), w(eeks), y(ears)], default is 1 month
    retentionPeriod: 7d
    # -- Sts/Deploy additional labels
    extraLabels: {}
    # -- Pod's additional labels
    podLabels: {}
    # -- Pod's annotations
    podAnnotations:
      prometheus.io/port: "8428"
      prometheus.io/scrape: "true"
    # -- Name of Priority Class
    priorityClassName:

    service:
      # -- Service annotations
      annotations: {}
      # -- Service labels
      labels: {}
      # -- Service ClusterIP
      clusterIP: None

    matchLabels: {}

    statefulSet:
      annotations: {}
      # -- Headless service labels
      labels: {}

    persistentVolume:
      # enabled: true
      # -- Persistant volume annotations
      annotations: {}

      # -- StorageClass to use for persistent volume. Requires server.persistentVolume.enabled: true. If defined, PVC created automatically
      storageClass:

      # -- Use this to override the prefix for the pvc, the suffix is auto-generated by k8s according to the pod name
      #pvcNameOverride:

      size: 100Gi
    extraVolumes:
      - name: extra-config
        configMap:
          name: victoria-metrics-aggregation-config
    extraVolumeMounts:
      - name: extra-config
        mountPath: "/extra-config"
        readOnly: true

    resources:
      requests:
        cpu: 1000m
        memory: 3000Mi
      limits:
        memory: 3000Mi

    # -- Node tolerations for server scheduling to nodes with taints. Ref: [https://kubernetes.io/docs/concepts/configuration/assign-pod-node/](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)
    tolerations: []

    # -- Pod's node selector. Ref: [https://kubernetes.io/docs/user-guide/node-selection/](https://kubernetes.io/docs/user-guide/node-selection/)
    nodeSelector: {}

    # -- Pod affinity
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: eks.amazonaws.com/capacityType
                  operator: NotIn
                  values:
                    - SPOT

monitors-manager:
  initChownData:
    image:
      tag: "1.36.1"
  image:
    registry: public.ecr.aws
    repository: groundcovercom/grafana-groundcover
    tag: v0.0.21-grafana11.2.0
  nameOverride: monitors-manager
  persistence:
    enabled: true
    type: statefulset
    size: 5Gi
  envValueFrom:
    API_KEY:
      secretKeyRef:
        key: '{{ include "groundcover.apikeySecretKey" . }}'
        name: '{{ include "groundcover.apikeySecretName" . }}'
    CLICKHOUSE_PASSWORD:
      secretKeyRef:
        key: '{{ include "clickhouse.secretKey" . }}'
        name: '{{ include "clickhouse.secretName" . }}'
    GF_DATABASE_PASSWORD:
      secretKeyRef:
        name: '{{ include "postgresql.secretName" . }}'
        key: '{{ include "postgresql.adminPasswordKey" . }}'
    KEEP_WEB_HOOK_API_KEY:
      secretKeyRef:
        optional: true
        key: webhook-key
        name: keep-credentials
  extraConfigmapMounts:
    - name: alerting-overrides
      mountPath: /etc/grafana/provisioning/alerting/
      configMap: alerting-overrides-config
      readOnly: true
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: VM
          type: prometheus
          uid: victoria-metrics
          url: '{{ include "victoria-metrics.base.http.url" . }}'
          access: proxy
          jsonData:
            prometheusType: "Prometheus"
            prometheusVersion: "2.50.1"
        - name: Clickhouse
          type: grafana-clickhouse-datasource
          uid: clickhouse
          url: '{{ include "clickhouse.shard0Name" . }}:{{ .Values.global.clickhouse.containerPorts.tcp | int }}'
          jsonData:
            defaultDatabase: '{{ include "clickhouse.database" . }}'
            port: "{{ .Values.global.clickhouse.containerPorts.tcp | int }}"
            server: '{{ include "clickhouse.shard0Name" . }}'
            username: '{{ include "clickhouse.username" . }}'
          access: proxy
          secureJsonData:
            password: $CLICKHOUSE_PASSWORD
  alertingOverrides:
      contactPoints:
        - name: workflows-webhook
          receivers:
            - uid: workflows-webhook
              type: webhook
              settings:
                url: '{{ include "keep.event.alert.url" . }}'
                httpMethod: "POST"
                authorization_scheme: digest
                authorization_credentials: $KEEP_WEB_HOOK_API_KEY
      policies:
      - receiver: workflows-webhook
        group_by:
          - '...'
        group_wait: 30s
        group_interval: 1m
        repeat_interval: 4h
  grafana.ini:
    groundcover:
      alerting_overrides_checksum: '{{ .Values.alertingOverrides | quote | sha256sum }}'
      workflows_enabled: '{{ .Values.global.workflows.enabled }}'
    "unified_alerting.state_history":
      enabled: true
      backend: "loki"
      loki_remote_url: '{{ include "opentelemetry-collector.loki-historian.http.url" . }}'
      log_all: true
      otel_export_enabled: "{{  .Values.global.vector.enabled }}"
      otel_endpoint: '{{ include "ingestion.monitors.otlp.http.url" . }}'
      otel_enable_tls: false
      otel_tls_skip_verify: true
    feature_toggles:
      enable: "alertStateHistoryLokiSecondary, alertStateHistoryLokiPrimary, alertStateHistoryLokiOnly"
    server:
      enable_gzip: true
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /usr/share/grafana/plugins
      provisioning: /etc/grafana/provisioning
    database:
      type: postgres
      host: '{{ (include "postgresql.base.url" .) }}'
      name: monitors_manager
      user: postgres
      ssl_mode: require
  extraInitContainers:
    - name: wait-for-db
      image: '{{ include "postgresql.image" . }}'
      env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: '{{ include "postgresql.secretName" . }}'
              key: '{{ include "postgresql.adminPasswordKey" . }}'
      command:
        - /bin/sh
        - -c
      args:
        - |
          pg_isready \
            -U "postgres" \
            -d "dbname=postgres" \
            -h {{ splitList ":" (include "postgresql.base.url" .) | first }} \
            -p {{ splitList ":" (include "postgresql.base.url" .) | last }}
  command:
    - "sh"
    - "-c"
    - "grafana cli --homepath=\"${GF_PATHS_HOME}\" --config=\"${GF_PATHS_CONFIG}\" admin reset-admin-password \"${GF_SECURITY_ADMIN_PASSWORD}\" && /run.sh"

curl:
  image:
    repository: '{{ .Values.global.origin.registry }}/curl' 
    tag: 8.6.0

rbac:
  pspEnabled: false
  sccEnabled: true
  labels:
  annotations:

apikey:
  labels:
  annotations:

config:
  labels:
  annotations:

backend:
  postgresql:
    dbs:
      monitors-manager: monitors_manager
      keep: keep
  keep:
    backend:
      resources:
        limits:
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi

groundcover:
  extraObjects: {}
