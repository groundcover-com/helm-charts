global:
  agent:
    enabled: true
  backend:
    enabled: true

  ingress:
    # set ingress site for inCloud / on-prem sensor only deployments
    site:

  groundcover_token:
  # groundcover_token is preceding groundcoverPredefinedTokenSecret, make sure its empty if using existing secret
  # if the secret is preloaded in the namespace, you can refer it here instead
  # example for a preloaded secret :
  # apiVersion: v1
  # kind: Secret
  # metadata:
  #   name: <secretName>
  # stringData:
  #   <secretKey>: <apikey>
  # type: Opaque
  groundcoverPredefinedTokenSecret:
    # the name of the secret
    secretName:
    # the key in the secret containing the token value
    secretKey:

  origin:
    registry: public.ecr.aws/groundcovercom
    tag: "" # rewrites Chart.AppVersion
  imagePullSecrets: []

  groundcoverLabels:

  clickhouse:
    auth:
      # override to use exisiting secret
      existingSecret: ""
      existingSecretKey: admin-password

  logs:
    overrideUrl: ""
    retention: 3d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  metrics:
    tls:
      enabled: false
    overrideUrl: ""

  otlp:
    tls:
      enabled: false
    overrideGrpcURL: ""
    overrideHttpURL: ""

  datadogapm:
    overrideUrl: ""

  traces:
    retention: 24h # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  events:
    retention: 7d # {amount}[h(ours), d(ays), w(eeks), y(ears)]

  telemetry:
    enabled: true
    logs:
      url: https://logs.groundcover.com/v1/logs
    metrics:
      url: https://metrics.groundcover.com/api/v1/write
    traces:
      otlpUrl: https://traces.groundcover.com/v1/traces
      zipkinUrl: https://traces.groundcover.com/api/v2/spans

tags:
  # InCloud - Enterprise Setup. groundcover's control-plane reconciled observability infrastructure deployment.
  incloud: false

# optional parameters: stable, legacy, experimental
mode: stable

# SAAS parameters, token is mendatory, others only if in cloud mode
saas:
  tls_skip_verify: false
  scheme: wss
  host: app.groundcover.com
  port: 443
  basePath: ""

#GENERAL
clusterId:
installationId:
# multipleClusterIds:
#  - clusterA
#  - clusterB
region:
dropRunningNamespaceLogs: true
storeIssuesLogsOnly: false
tracesNamespaceFilters:
  []
  # - matchType: "allow"
  #   regex: "my-namespace"
tracesWorkloadFilters:
  []
  # - matchType: "block"
  #   regex: "do-not-show-me"
logsDropFilters: []
# - '{namespace="demo-ng",workload="loadgenerator"} |~ ".*GET.*"'
logsMultiLines:
  []
  # - namespace: demo-ng             # mandatory
  #   workload: ".*"                 # mandatory
  #   container: "test"              # optional
  #   firstLineRegex: "PlaceOrder"   # mandatory
  #   maxLines: 100                  # optional, default = 1024
  #   maxWaitTime: 10s               # optional, default = 3s
decolorizeLogs: false
shouldDropRunningNamespaces: true
commitHashKeyName:
repositoryUrlKeyName:
dropGroundcoverCustomMetrics: true

priorityClass:
  create: false
  name:
  value:
  preemptionPolicy:

#imagePullSecrets: []

volume-expansion:
  enabled: true
  image:
    tag: latest
    repository: "{{ .Values.global.origin.registry }}/bitnami/kubectl"

agent:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  tolerations: []
  affinity:
  nodeSelector:
  priorityClassName:
  hostNetwork: false
  monitoring:
    port: 9102
  flb:
    image:
      tag: 2.1.4
      repository: "{{ .Values.global.origin.registry }}/fluent/fluent-bit"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 64Mi
  alligator:
    image:
      repository: "{{ .Values.global.origin.registry }}/alligator"
    resources:
      requests:
        memory: 300Mi
        cpu: 160m
      limits:
        memory: 700Mi
        cpu: 800m
    dataRetention: 24h
    nodelabels: []
    contentTypesToDrop: []
    watchOnlyLocalNode: false
    apmIngestor:
      dataDog:
        proxyEnabled: true
        tracesPort: 8126
      otel:
        proxyEnabled: false
        direct:
          enabled: true
          zipkin:
            enabled: true
            port: 9411
          otlp:
            enabled: true
            grpcPort: 4317
    pprofAlligator:
      enabled: true
      interval: 10s
      exponent: 40
      uploaderType: pyroscope
      cpuSamplingDuration: 60s
    httphandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        enabled: false
    grpchandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        enabled: false
    redishandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        enabled: false
    sqlhandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        enabled: false
    kafkahandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        enabled: false
    dnshandler:
      truncationConfig:
        enabled: false
      obfuscationConfig:
        enabled: false
    mongodbhandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        enabled: false
    noPayloadsMode: false
    env:
  tracy:
    image:
      repository: "{{ .Values.global.origin.registry }}/tracy"
    resources:
      requests:
        memory: 512Mi
        cpu: 125m
      limits:
        memory: 2048Mi
        cpu: 750m
dbManager:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  image:
    repository: "{{ .Values.global.origin.registry }}/db-manager"
  resources:
    limits:
      cpu: 100m
      memory: 1Gi
    requests:
      cpu: 15m
      memory: 15Mi
  extraEnvVars:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'

k8sWatcher:
  image:
    repository: "{{ .Values.global.origin.registry }}/k8s-watcher"
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  resources:
    limits:
      cpu: 1000m
      memory: 1024Mi
    requests:
      cpu: 50m
      memory: 256Mi
  env:

portal:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  env:
  extraHeaders:
  # - Key: "Header Key"
  #   Value: "Header Value"
  image:
    repository: "{{ .Values.global.origin.registry }}/portal"
  resources:
    limits:
      cpu: 100m
      memory: 200Mi
    requests:
      cpu: 50m
      memory: 100Mi

clickhouse:
  enabled: true
  shards: 1
  replicaCount: 1
  image:
    registry: public.ecr.aws/groundcovercom
  containerSecurityContext:
    enabled: false
  podLabels:
    app.kubernetes.io/part-of: groundcover
  podSecurityContext:
    enabled: false
  volumePermissions:
    enabled: true
    image:
      registry: public.ecr.aws/groundcovercom
  zookeeper:
    enabled: false
  metrics:
    enabled: true
  auth:
    existingSecret: "false" # override global.clickhouse.auth.existingSecret to use exisiting secret
  extraEnvVars:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  extraOverrides: |
    <clickhouse>
      <max_table_size_to_drop>0</max_table_size_to_drop>
      <max_partition_size_to_drop>0</max_partition_size_to_drop>
      <profiles>
        <default>
          <async_insert>true</async_insert>
          <max_execution_time>60</max_execution_time>
          <wait_for_async_insert>true</wait_for_async_insert>
        </default>
      </profiles>
      <query_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </query_log>
      <trace_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </trace_log>
      <metric_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </metric_log>
    </clickhouse>
  usersExtraOverrides: |
    <clickhouse>
      <profiles>
        <readonly_with_settings>
          <readonly>2</readonly>
        </readonly_with_settings>
      </profiles>
      <users>
        <reader>
          <password from_env="CLICKHOUSE_PASSWORD" />
          <networks>
            <ip>::/0</ip>
          </networks>
          <profile>readonly_with_settings</profile>
          <quota>default</quota>
        </reader>
      </users>
    </clickhouse>
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.labels Persistent Volume Claim labels
    ##
    labels:
      app.kubernetes.io/managed-by: Helm
      helm.sh/chart: clickhouse-3.2.1
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 256Gi

  ## ClickHouse resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ## @param resources.limits The resources limits for the ClickHouse containers
  ## @param resources.requests The requested resources for the ClickHouse containers
  ##
  resources:
    requests:
      cpu: 300m
      memory: 2048Mi
    limits:
      memory: 4096Mi

  ## @param affinity Affinity for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `podAffinityPreset`, `podAntiAffinityPreset`, and `nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param updateStrategy.type ClickHouse statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##

  extraVolumes:
    - name: clickhouse-client-config
      configMap:
        name: clickhouse-client-config-map

  extraVolumeMounts:
    - name: clickhouse-client-config
      mountPath: /etc/clickhouse-client/

opentelemetry-collector:
  enabled: true
  mode: statefulset
  statefulset:
    podManagementPolicy: "OrderedReady"
  extraEnvs:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
    - name: TLS_CONFIG
      value: '{{ include "opentelemetry-collector.tlsConfig" . }}'
    - name: CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
  extraVolumes:
    - name: certificate
      secret:
        secretName: opentelemetry-collector-certificate
    - name: persistent-queues
      emptyDir: {}
  extraVolumeMounts:
    - name: certificate
      readOnly: true
      mountPath: /etc/ssl/certs
    - name: persistent-queues
      mountPath: /persistent_queues
  image:
    repository: public.ecr.aws/groundcovercom/otel/opentelemetry-collector-contrib
    tag: ""
  podLabels:
    app.kubernetes.io/part-of: groundcover
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: '{{ index .Values "ports" "metrics" "containerPort" | int }}'
  ports:
    metrics:
      enabled: true
    pprof:
      enabled: true
      containerPort: 1777
      servicePort: 1777
      hostPort: 1777
      protocol: TCP
    faro:
      enabled: true
      containerPort: 12347
      servicePort: 12347
      hostPort: 12347
      protocol: TCP
    loki-http:
      enabled: true
      containerPort: 3100
      servicePort: 3100
      hostPort: 3100
      protocol: TCP
    health:
      enabled: true
      containerPort: 13133
      servicePort: 13133
      hostPort: 13133
      protocol: TCP
    datadogapm:
      enabled: true
      containerPort: 8126
      servicePort: 8126
      hostPort: 8126
      protocol: TCP
  config:
    extensions:
      health_check:
        path: /health
        tls: ${env:TLS_CONFIG}
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "health" "containerPort" | int ) }}'
      pprof:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "pprof" "containerPort" | int ) }}'
      file_storage:
        directory: /persistent_queues
        timeout: 10s
    receivers:
      loki:
        protocols:
          http:
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "loki-http" "containerPort" | int ) }}'
        use_incoming_timestamp: true
      otlp:
        protocols:
          grpc:
            max_recv_msg_size_mib: 50
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "otlp" "containerPort" | int ) }}'
          http:
            tls: ${env:TLS_CONFIG}
            cors:
              allowed_origins:
                - "*"
              allowed_headers:
                - "*"
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "otlp-http" "containerPort" | int ) }}'
      faro:
        http:
          tls: ${env:TLS_CONFIG}
          cors:
            allowed_origins:
              - "*"
            allowed_headers:
              - "*"
          endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "faro" "containerPort" | int ) }}'
      datadog:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "datadogapm" "containerPort" | int ) }}'
    processors:
      batch/logs:
        timeout: 5s
        send_batch_size: 20000
      batch/custom:
        timeout: 5s
        send_batch_size: 1000
      batch/traces:
        timeout: 5s
        send_batch_size: 5000
      filter/dropNoneCustom:
        error_mode: ignore
        logs:
          log_record:
            # custom logs have explicit log_type or type so log is excluded
            - attributes["log_type"] == "log"
            - attributes["type"] == nil
      filter/dropNoneLogs:
        logs:
          log_record:
            # logs can have either explicit 'log' log_type or no log_type at all and no type (reserved for custom logs)
            - attributes["log_type"] != nil and attributes["log_type"] != "log"
            - attributes["type"] != nil
    exporters:
      groundcover/logs:
        type: logs
        timeout: 5s
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        ttl_interval: '{{ include "logs.retention" . }}'
        sending_queue:
          queue_size: 10
          storage: file_storage
        retry_on_failure:
          initial_interval: 5s
          max_interval: 5s
          max_elapsed_time: 15s
      groundcover/custom:
        type: custom
        timeout: 5s
        ttl_interval: 5d
        logs_table_name: k8s_objects
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        sending_queue:
          queue_size: 10
          storage: file_storage
        retry_on_failure:
          initial_interval: 5s
          max_interval: 5s
          max_elapsed_time: 15s
      groundcover/traces:
        type: traces
        timeout: 5s
        ttl_interval: '{{ include "traces.retention" . }}'
        traces_table_name: traces
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        sending_queue:
          queue_size: 10
          storage: file_storage
        retry_on_failure:
          initial_interval: 5s
          max_interval: 5s
          max_elapsed_time: 15s
    service:
      telemetry:
        metrics:
          address: '{{ printf "0.0.0.0:%d" (index .Values "ports" "metrics" "containerPort" | int ) }}'
      extensions: [health_check, pprof, file_storage]
      pipelines:
        metrics:
          receivers:
            - otlp
        traces:
          receivers:
            - otlp
            - datadog
            - faro
          processors:
            - batch/traces
          exporters:
            - groundcover/traces
        logs:
          receivers:
            - loki
            - otlp
            - faro
          processors:
            - filter/dropNoneLogs
            - batch/logs
          exporters:
            - groundcover/logs
        logs/custom:
          receivers:
            - otlp
          processors:
            - filter/dropNoneCustom
            - batch/custom
          exporters:
            - groundcover/custom
  initContainers:
    - args:
        - while [ $(curl -sw '%{http_code}' {{ printf "%s?gc-version=%s " (include "db-manager.ready.http.url" .) (default .Chart.AppVersion .Values.global.origin.tag) }}
          -o /dev/null) -ne 200 ]; do echo 'Waiting for Db Manager...'; sleep 2; done; echo Db Manager is ready
      command:
        - /bin/sh
        - -c
      image: '{{ printf "%s/curl:latest" .Values.global.origin.registry }}'
      name: check-db-ready
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 2048Mi
  affinity: {}
  nodeSelector: {}
  tolerations: []

router:
  # onprem or cloud
  mode: cloud
  enabled: false
  additionalLabels:
  additionalAnnotations:

metrics-ingester:
  nodeSelector:
  rbac:
    enabled: false
  service:
    enabled: true
  podAnnotations:
    prometheus.io/port: "8429"
    prometheus.io/scrape: "true"
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  extraArgs:
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    remoteWrite.showURL: "true"
    remoteWrite.maxDiskUsagePerURL: 1GB
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.tlsInsecureSkipVerify: "true"
    remoteWrite.streamAggr.config: "/extra-config/aggregation_config.yaml"
    remoteWrite.url: '{{ include "victoria-metrics.write.http.url" . }}'
    tlsKeyFile: /etc/ssl/certs/tls.key
    tlsCertFile: /etc/ssl/certs/tls.crt
    tls: "{{ .Values.global.metrics.tls.enabled }}"
  env:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  extraVolumes:
    - name: certificate
      secret:
        optional: true
        secretName: metrics-ingester-certificate
    - name: extra-config
      configMap:
        name: metrics-ingester-aggregation-config
  extraVolumeMounts:
    - name: certificate
      readOnly: true
      mountPath: /etc/ssl/certs
    - name: extra-config
      readOnly: true
      mountPath: /extra-config
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 256Mi
  config:
    scrape_configs: []

victoria-metrics-agent:
  nodeSelector:
  rbac:
    namespaced: true
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  podAnnotations:
    prometheus.io/port: "8429"
    prometheus.io/scrape: "true"
  # imagePullSecrets:
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  extraArgs:
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    remoteWrite.showURL: "true"
    remoteWrite.maxDiskUsagePerURL: 1GB
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.url: '{{ include "telemetry.metrics.url" . }}'
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID),groundcover_version=$(GC_VERSION)
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
    - name: GC_VERSION
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_VERSION
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  config:
    scrape_configs:
      - job_name: groundcover
        kubernetes_sd_configs:
          - namespaces:
              own_namespace: true
            role: pod
        relabel_configs:
          - action: keep
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - action: drop
            regex: true
            source_labels:
              - __meta_kubernetes_pod_container_init
          - action: keep_if_equal
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_port
              - __meta_kubernetes_pod_container_port_number
          - action: keep
            regex: true
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scrape
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scheme
            target_label: __scheme__
          - action: replace
            regex: (.+)
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_path
            target_label: __metrics_path__
          - action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            source_labels:
              - __address__
              - __meta_kubernetes_pod_annotation_prometheus_io_port
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - action: replace
            source_labels:
              - __meta_kubernetes_namespace
            target_label: kubernetes_namespace
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_name
            target_label: kubernetes_pod_name
          - source_labels:
              - __meta_kubernetes_pod_name
            target_label: instance
        tls_config:
          insecure_skip_verify: true

kube-state-metrics:
  nodeSelector:
  nameOverride: kube-state-metrics
  customLabels:
    app.groundcover.com/owner: groundcover
  service:
    port: 8080
    annotations:
      prometheus.io/port: "8080"
  collectors:
    # - certificatesigningrequests
    - configmaps
    - cronjobs
    - daemonsets
    - deployments
    # - endpoints
    - horizontalpodautoscalers
    # - ingresses
    - jobs
    # - leases
    # - limitranges
    # - mutatingwebhookconfigurations
    # - namespaces
    # - networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    # - poddisruptionbudgets
    - pods
    - replicasets
    - replicationcontrollers
    # - resourcequotas
    # - secrets
    # - services
    - statefulsets
  # - storageclasses
  # - validatingwebhookconfigurations
  # - volumeattachments

victoria-metrics-operator:
  enabled: false
  builtinVMAgent:
    enabled: false
    spec:
      podScrapeSelector: {}
      serviceScrapeSelector: {}
      nodeScrapeSelector: {}
      staticScrapeSelector: {}
      probeSelector: {}
      replicaCount: 1
      remoteWrite:
        - url: '{{ include "custom-metrics.write.http.url" . }}'

custom-metrics:
  enabled: false
  service:
    enabled: true
  nodeSelector:
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8429"
  rbac:
    namespaced: false
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  extraArgs:
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    remoteWrite.showURL: "true"
    remoteWrite.maxDiskUsagePerURL: 1GB
    remoteWrite.maxDailySeries: "1000000"
    remoteWrite.maxHourlySeries: "100000"
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.tlsInsecureSkipVerify: "true"
    remoteWrite.url: '{{ include "victoria-metrics.write.http.url" . }}'
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID)
    remoteWrite.relabelConfig: "/extra-config/global-relabel-config.yaml"
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  extraVolumes:
    - name: extra-config
      configMap:
        name: custom-metrics-global-relabel-config
  extraVolumeMounts:
    - name: extra-config
      readOnly: true
      mountPath: /extra-config
  extraScrapeConfigs: []

victoria-metrics-single:
  # imagePullSecrets: []
  server:
    image:
      repository: public.ecr.aws/groundcovercom/victoria-metrics

    extraArgs:
      search.maxUniqueTimeseries: "2000000"
      search.maxSeries: "500000"
      search.maxTagKeys: "200000"
      search.maxTagValues: "500000"
      search.maxQueryLen: "100000"
      search.maxConcurrentRequests: "8"
      maxLabelsPerTimeseries: "50"
      promscrape.maxScrapeSize: "100MB"

    # -- Data retention period, {amount}[h(ours), d(ays), w(eeks), y(ears)], default is 1 month
    retentionPeriod: 7d
    # -- Sts/Deploy additional labels
    extraLabels: {}
    # -- Pod's additional labels
    podLabels: {}
    # -- Pod's annotations
    podAnnotations:
      prometheus.io/port: "8428"
      prometheus.io/scrape: "true"
    # -- Name of Priority Class
    priorityClassName:

    service:
      # -- Service annotations
      annotations: {}
      # -- Service labels
      labels: {}
      # -- Service ClusterIP
      clusterIP: None

    matchLabels: {}

    statefulSet:
      annotations: {}
      # -- Headless service labels
      labels: {}

    persistentVolume:
      # enabled: true
      # -- Persistant volume annotations
      annotations: {}

      # -- StorageClass to use for persistent volume. Requires server.persistentVolume.enabled: true. If defined, PVC created automatically
      storageClass:

      # -- Use this to override the prefix for the pvc, the suffix is auto-generated by k8s according to the pod name
      #pvcNameOverride:

      size: 100Gi

    resources:
      requests:
        cpu: 1000m
        memory: 3000Mi
      limits:
        cpu: 1000m
        memory: 3000Mi

    # -- Node tolerations for server scheduling to nodes with taints. Ref: [https://kubernetes.io/docs/concepts/configuration/assign-pod-node/](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)
    tolerations: []

    # -- Pod's node selector. Ref: [https://kubernetes.io/docs/user-guide/node-selection/](https://kubernetes.io/docs/user-guide/node-selection/)
    nodeSelector: {}

    # -- Pod affinity
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: eks.amazonaws.com/capacityType
                  operator: NotIn
                  values:
                    - SPOT

rbac:
  pspEnabled: false
  sccEnabled: true
  labels:
  annotations:

apikey:
  labels:
  annotations:

config:
  labels:
  annotations:
