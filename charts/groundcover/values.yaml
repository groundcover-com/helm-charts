global:
  agent:
    enabled: true
  integrations:
    agent:
      enabled: false
  backend:
    name:
    enabled: true
    victoria-metrics-distributed:
      enabled: false
  debug:
    enabled: false
  workflows:
    enabled: true
  vector:
    forceLocal: false
    tracesAsLogs:
      otlp:
        overrideHttpURL:
    logs:
      otlp:
        overrideHttpURL:
    custom:
      otlp:
        overrideHttpURL:
    health:
      overrideHttpURL:
  fleetmanager:
    enabled: false
  ingestor:
    enabled: false

  ingress:
    # set ingress site for inCloud / on-prem sensor only deployments
    site:
    extraSites: []

  airgap: false

  ingestion:
    tls_skip_verify: true
    pdbs:
      enabled: true
  groundcover_token:
  # groundcover_token is preceding groundcoverPredefinedTokenSecret, make sure its empty if using existing secret
  # if the secret is preloaded in the namespace, you can refer it here instead
  # example for a preloaded secret :
  # apiVersion: v1
  # kind: Secret
  # metadata:
  #   name: <secretName>
  # stringData:
  #   <secretKey>: <apikey>
  # type: Opaque
  groundcoverPredefinedTokenSecret:
    # the name of the secret
    secretName:
    # the key in the secret containing the token value
    secretKey:

  origin:
    registry: public.ecr.aws/groundcovercom
    tag: "" # rewrites Chart.AppVersion
  imagePullSecrets: []

  groundcoverPartOf: groundcover
  groundcoverLabels:

  clickhouse:
    auth:
      # override to use exisiting secret
      existingSecret: ""
      existingSecretKey: admin-password

  logs:
    overrideUrl: ""
    retention: 3d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    cold_move_duration: 7d

  metrics:
    tls:
      enabled: false
    overrideUrl: ""
    write:
      destinations:
        backend:
          url: '{{ include "victoria-metrics.write.http.url" . }},{{ include "vector.cluster.prometheus_remote_write.vm_metrics.endpoint" . }}'
          rateLimit: '0,500000'
          disableOnDiskQueue: 'false,false'
          maxDiskUsagePerURL: '10GB,512MB'
          forceVMProto: "true,false"
          forcePromProto: "false,true"
        sensor:
          url: '{{ include "victoria-metrics.write.http.url" . }}'
          rateLimit: '0'
          disableOnDiskQueue: 'false'
          maxDiskUsagePerURL: '10GB'
          forceVMProto: "true"
          forcePromProto: "false"

  otlp:
    overrideGrpcURL: ""
    overrideHttpURL: ""

  datadogapm:
    overrideUrl: ""

  traces:
    retention: 24h # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    cold_move_duration: 7d

  events:
    retention: 7d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    cold_move_duration: 7d

  measurements:
    retention: 7d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    cold_move_duration: 7d

  sources:
    retention: 30d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    cold_move_duration: 7d

  entities:
    retention: 5d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    cold_move_duration: 7d

  metricsMetadata:
    retention: 30d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
    cold_move_duration: 7d

  telemetry:
    enabled: true
    logs:
      url: https://logs.groundcover.com/v1/logs
    metrics:
      url: https://metrics.groundcover.com/api/v1/write
      interval: 30s
    traces:
      otlpUrl: https://traces.groundcover.com/v1/traces
      zipkinUrl: https://traces.groundcover.com/api/v2/spans

  monitors:
    evaluation:
      retention: 30d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
      cold_move_duration: 7d
    instance:
      retention: 90d # {amount}[h(ours), d(ays), w(eeks), y(ears)]
      cold_move_duration: 7d

  postgresql:
    enabled: true
    overrideUrl: ""

tags:
  # InCloud - Enterprise Setup. groundcover's control-plane reconciled observability infrastructure deployment.
  incloud: false
  managed: true
  
# SAAS parameters, token is mendatory, others only if in cloud mode
saas:
  tls_skip_verify: '{{ include "portal.saas.tls_skip_verify" . }}'
  scheme: '{{ include "portal.saas.scheme" . }}'
  host: '{{ include "portal.saas.host" . }}'
  port: '{{ include "portal.saas.port" . }}'
  basePath: ""

#GENERAL
env:
clusterId:
env_type: k8s
installationId:
# multipleClusterIds:
#  - clusterA
#  - clusterB
region:
dropRunningNamespaceLogs: 
tracesNamespaceFilters:
  []
  # - matchType: "allow"
  #   regex: "my-namespace"
tracesWorkloadFilters:
  []
  # - matchType: "block"
  #   regex: "do-not-show-me"
logsDropFilters: []
# - '{namespace="demo-ng",workload="loadgenerator"} |~ ".*GET.*"'
logsMultiLines:
  []
  # - namespace: demo-ng             # mandatory
  #   workload: ".*"                 # mandatory
  #   container: "test"              # optional
  #   firstLineRegex: "PlaceOrder"   # mandatory
  #   maxLines: 100                  # optional, default = 1024
  #   maxWaitTime: 10s               # optional, default = 3s
grokParserConfig:
  - containers:
      - "kong"
      - "nginx"
    pattern: '(?<timestamp>%{YEAR}[./]%{MONTHNUM}[./]%{MONTHDAY} %{TIME}) \[%{LOGLEVEL:level}\] %{POSINT:pid}(?:#%{NUMBER:tid})?: (?:\*%{NUMBER:connectionid} )?(?<message>[^,]+)(?:, excess: %{NUMBER:excess})?(?:, by zone "%{DATA:zone}")?(?:, client: %{IP:client})?(?:, server: (%{HOSTNAME:server}|))?(?:, request: "%{DATA:request}")?(?:, host: "%{DATA:host}")?(?:, refer(?:rer)?: "%{DATA:referer}")?'
    ruleName: "nginx_error"
  - containers:
      - "postgresql"
    pattern: '%{TIMESTAMP_ISO8601:timestamp} %{WORD:timezone} \[%{DATA:user_id}\] +%{GREEDYDATA:message}'
    ruleName: "postgresql"
ottlRules:
  []
  # - ruleName: "test"
  #   statements:
  #     - set(attributes["test"], "test_value")

isk8s: true
fetchKubeletInfraMetrics: false
sendKubeletInfraMetrics: true
positionsFile: "/var/run/positions/positions.yaml"
decolorizeLogs: false
maxLogSize: 102400 # 100KB
maxLogContentSize: 5120 # 5KB
logBatchSendQueueWorkerCount: 3
logBatchSendQueueMaxSize: 10
oldLogDropThreshold: 180s
oldLogDropAlwaysThreshold: 60m
oldLogDelayStart: 60s
jsonFlattenMaxDepth: 6
jsonFlattenMaxAttributes: 300
shouldDropRunningNamespaces: 
commitHashKeyName:
repositoryUrlKeyName:
logPatternsConfig:
  enabled: true
  logClusterDepth: 6
  similarityThreshold: 0.6
  maxChildren: 20
  maxClusters: 2000
  paramString: "<*>"
  maxEvictionRatio: 0.8
  maxTokens: 50
  maxContentLength: 1024
  containerIdCacheMaxSize: 500
  containerIdCacheTTL: 15m
  summarizerConfig:
    flushInterval: 30s
    workloadCacheSize: 1000
    patternsTopCount: 20

fleetClientConfig:
  overrideURL: ""
  requestIntervalEnabled: true
  maximumInitialJitter: 30s
  requestInterval: 30s
  initialUpdateTimeout: 60s # setting value to 0 means sensor will not wait for initial update
priorityClass:
  create: false
  name:
  value:
  preemptionPolicy:

#imagePullSecrets: []

statefulset-modifier:
  enabled: true
  sizePatches: true
  image:
    tag: latest
    repository: "{{ .Values.global.origin.registry }}/bitnami/kubectl"

agent:
  randomSuffixName: false
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  tolerations:
  - operator: "Exists"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: eks.amazonaws.com/compute-type
            operator: NotIn
            values:
            - fargate
  nodeSelector:
  priorityClassName:
  hostNetwork: false
  monitoring:
    port: 9102
  initContainers:  
    checkIngestion:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}' 
    checkMetrics:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}' 
  flb:
    enabled: true
    # for ipv6, change to "[::]"
    bindAddress: 0.0.0.0
    image:
      tag: 2.2.3
      repository: "{{ .Values.global.origin.registry }}/fluent/fluent-bit"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 64Mi
    env: []
    builtinEnv:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ template "groundcover.apikeySecretKey" . }}'
          name: '{{ template "groundcover.apikeySecretName" . }}' 
  sensor:
    image:
      repository: "{{ .Values.global.origin.registry }}/sensor"
      tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
    resources:
      requests:
        memory: 300Mi
        cpu: 160m
      limits:
        memory: 1Gi
        cpu: 1000m
    nodelabels: []
    contentTypesToDrop: []
    contentTypesWithoutClustering: []
    hostHeadersToDrop: []
    headersForceSampling:
      gcForceSample: true
      w3cForceSample: false
      b3ForceSample: false
      datadogForceSample: false
    watchOnlyLocalNode: false
    setLocalTrafficPolicy: true
    collectionEnabled: true
    ingestionEnabled: true
    customEntityTags:
      labels:
        keys: []
        collectAll: false
      annotations:
        keys: []
        collectAll: false
    healthProbe:
      enabled: true
      port: 2021
    metricIngestor:
      serverEnabled: true
      serverPort: 8428
      scraperEnabled: false
      maxScrapeSize: 67108864
      usePrometheusCompatibleNaming: true
    apmIngestor:
      dataDog:
        enabled: true
        proxyEnabled: false
        tracesPort: 8126
        samplingRatio: 0.05
      otel:
        proxyEnabled: false
        direct:
          enabled: true
          samplingRatio: 0.05
          zipkin:
            enabled: true
            port: 9411
          otlp:
            enabled: true
            grpcPort: 4317
            httpPort: 4318
            maxRecvMsgSizeMiB: 50
          jaeger:
            enabled: true
            grpcPort: 14250
            thriftHttpPort: 14268
            thriftBinaryPort: 6832
            thriftCompactPort: 6831
          httpjson:
            enabled: true
            port: 8016
          firehose:
            enabled: false
            port: 4433
    pprof:
      enabled: true
      uploaderType: pyroscope
      cpuSamplingDuration: 60s
      backoffTicker:
        interval: 10s
        exponent: 40
      memoryTicker:
        enabled: true
        interval: 60s
        jumpSizeInBytes: 100000000
        windowSize: 12h
        initialWait: 10m
      cpuTicker:
        enabled: true
        interval: 60s
        jumpSizeInMCPU: 100
        windowSize: 12h
        initialWait: 10m
    httphandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    grpchandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    redishandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
      obfuscateSensitiveValues: true
    sqlhandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    kafkahandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    dnshandler:
      truncationConfig:
        enabled: false
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    mongodbhandler:
      truncationConfig:
        enabled: true
        payloadSizeLimit: 10240
      obfuscationConfig:
        keyValueConfig:
        unstructuredConfig:
    noPayloadsMode: false
    k8sEntitiesWatch:
      crd:
        enabled: true
        allowedKinds:
          workflow:
            group: "argoproj.io"
            kindPlural: "workflows"
    sensitiveHeadersObfuscationConfig:
      enabled: true
      mode: "ObfuscateSpecificValues"
      specificKeys:
        [
          "Authorization",
          "Proxy-Authorization",
          "X-Amz-Security-Token",
          "X-Amz-Credential",
          "X-Amz-Firehose-Access-Key",
          "Cookie",
          "Set-Cookie",
          "X-Api-Key",
          "X-Csrf-Token",
          "X-XSRF-TOKEN",
          "X-Google-Api-Key",
          "X-Firebase-Auth"
        ]
    env:
    builtinEnv:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ template "groundcover.apikeySecretKey" . }}'
          name: '{{ template "groundcover.apikeySecretName" . }}' 
    httpUnlimitedEndpoints: "/api/v0.2/traces"
    http2UnlimitedEndpoints: "/api/v0.2/traces"
    childcountruleoverrides:
    - depth: 0
      maxChildren: 100
  priorityClass:
    create: true
    fullname:
    value: 1000000000
    preemptionPolicy: PreemptLowerPriority
dbManager:
  additionalLabels:
  podLabels:
  additionalAnnotations: {}
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  serviceAccount:
    name: ""
    annotations: {}
  image:
    repository: "{{ .Values.global.origin.registry }}/db-manager"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  resources:
    limits:
      memory: 1Gi
    requests:
      cpu: 15m
      memory: 15Mi
  extraEnvVars:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  objectStorageQueues: {}
  cloudPricing: 
    enabled: true
    pricingFileURL: "https://groundcover-public-assets.s3.amazonaws.com/cloud_pricing_v1.csv"
    refreshInterval: 6h # {amount}[h(ours), d(ays), w(eeks), y(ears)]

integrationsAgent:
  initContainers:
    checkIngestion:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}'
    checkMetrics:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}'   
  image:
    repository: "{{ .Values.global.origin.registry }}/integrations-agent"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  additionalLabels:
  podLabels:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  resources:
    limits:
      cpu: 2000m
      memory: 2048Mi
    requests:
      cpu: 500m
      memory: 256Mi
  pprof:
    enabled: true
    uploaderType: pyroscope
    cpuSamplingDuration: 60s
    backoffTicker:
      interval: 10s
      exponent: 40
    memoryTicker:
      enabled: true
      interval: 60s
      jumpSizeInBytes: 100000000
      windowSize: 12h
      initialWait: 10m
    cpuTicker:
      enabled: true
      interval: 60s
      jumpSizeInMCPU: 100
      windowSize: 12h
      initialWait: 10m
  env: []
  builtinEnv:
  - name: API_KEY
    valueFrom:
      secretKeyRef:
        key: '{{ template "groundcover.apikeySecretKey" . }}'
        name: '{{ template "groundcover.apikeySecretName" . }}'        
  readinessProbe:
    httpGet:
      scheme: HTTP
      path: /health
      port: agent-http
    initialDelaySeconds: 45
    periodSeconds: 15
    failureThreshold: 20
  livenessProbe:
    httpGet:
      scheme: HTTP
      path: /health
      port: agent-http
    initialDelaySeconds: 45
    periodSeconds: 15
    failureThreshold: 20
  extraVolumes: []
  extraVolumeMounts: []
  exporters:
    prometheus:
      requestTimeout: 30s
      batchSize: 2000
      maxRetries: 3
      maxDelay: 15s
      baseDelay: 5s
      metadata:
        maxRetries: 3
        maxDelay: 15s
        baseDelay: 5s
        compress: true
        cacheTTL: 1h
        cacheSize: 100000
        writeInterval: 10s
        maxBatchSize: 2000
        maxBatchWait: 10s
ingestor:
  image:
    repository: "{{ .Values.global.origin.registry }}/sensor"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  replicas: 2
  additionalLabels:
  additionalAnnotations:
  podLabels:
  podAnnotations:
  pdb:
    enabled: true
    maxUnavailable: 1
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: ingestor
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  monitoring:
    port: 9102
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      memory: 2048Mi
  builtinEnv:
  - name: API_KEY
    valueFrom:
      secretKeyRef:
        key: '{{ template "groundcover.apikeySecretKey" . }}'
        name: '{{ template "groundcover.apikeySecretName" . }}'  
  logLevel: 4
  healthProbe:
    enabled: true
    port: 2021
  pprof:
    enabled: '{{ include ".telemetry.enabled" . }}'
    uploaderType: pyroscope
    cpuSamplingDuration: 60s
    backoffTicker:
      interval: 10s
      exponent: 40
    memoryTicker:
      enabled: true
      interval: 60s
      jumpSizeInBytes: 100000000
      windowSize: 12h
      initialWait: 10m
    cpuTicker:
      enabled: true
      interval: 60s
      jumpSizeInMCPU: 100
      windowSize: 12h
      initialWait: 10m
  metricIngestor:
    serverEnabled: true
    serverPort: 8428
    scraperEnabled: false
    maxScrapeSize: 67108864
    usePrometheusCompatibleNaming: true
  collectionEnabled: false
  ingestionEnabled: true
  setLocalTrafficPolicy: false
  apmIngestor:
    dataDog:
      enabled: true
      proxyEnabled: false
      tracesPort: 8126
      samplingRatio: 0.05
    otel:
      proxyEnabled: false
      direct:
        enabled: true
        samplingRatio: 0.05
        zipkin:
          enabled: true
          port: 9411
        otlp:
          enabled: true
          grpcPort: 4317
          httpPort: 4318
          maxRecvMsgSizeMiB: 50
        jaeger:
          enabled: true
          grpcPort: 14250
          thriftHttpPort: 14268
          thriftBinaryPort: 6832
          thriftCompactPort: 6831
        httpjson:
          enabled: true
          port: 8016
        firehose:
          enabled: true
          port: 4433

fleetManager:
  image:
    repository: "{{ .Values.global.origin.registry }}/fleet-manager"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  replicas: 2
  additionalLabels:
  additionalAnnotations:
  podLabels:
  podAnnotations:
  pdb:
    enabled: true
    maxUnavailable: 1
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: fleet-manager
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  builtinEnv:
  - name: API_KEY
    valueFrom:
      secretKeyRef:
        key: '{{ template "groundcover.apikeySecretKey" . }}'
        name: '{{ template "groundcover.apikeySecretName" . }}'  
  logLevel: 4
  manageHttpPort: 8080
  healthProbePort: 8081
  pprof:
    enabled: '{{ include ".telemetry.enabled" . }}'
    uploaderType: pyroscope
    cpuSamplingDuration: 60s
    backoffTicker:
      interval: 10s
      exponent: 40
    memoryTicker:
      enabled: true
      interval: 60s
      jumpSizeInBytes: 100000000
      windowSize: 12h
      initialWait: 10m
    cpuTicker:
      enabled: true
      interval: 60s
      jumpSizeInMCPU: 100
      windowSize: 12h
      initialWait: 10m
  configManager:
    db:
      host: '{{ splitList ":" (include "postgresql.base.url" .) | first }}'
      port: '{{ splitList ":" (include "postgresql.base.url" .) | last }}'
      name: "fleetmanager"
      user: "postgres"
      pass: ""
      timeout: 150s
      interval: 5s
      sslmode: "require"
    skipMigrations: false
    migrationsPath: "/migrations"
    cacheRefreshInterval: 2s
    

k8sWatcher:
  initContainers:
    checkIngestion:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}'
    checkMetrics:
      builtinEnv:
      - name: API_KEY
        valueFrom:
          secretKeyRef:
            key: '{{ template "groundcover.apikeySecretKey" . }}'
            name: '{{ template "groundcover.apikeySecretName" . }}'   
  watch:
    cronjob:
      enabled: true
    configmap:
      enabled: true
    secret:
      enabled: false
    verticalPodAutoscaler:
      enabled: false
    crd:
      enabled: true
      allowedKinds:
        workflow:
          group: "argoproj.io"
          kindPlural: "workflows"
  image:
    repository: "{{ .Values.global.origin.registry }}/k8s-watcher"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  topologySpreadConstraints:
  nodeSelector:
  tolerations: []
  priorityClassName:
  clusterDiscoveryInterval: 5m
  resources:
    limits:
      cpu: 2000m
      memory: 1024Mi
    requests:
      cpu: 500m
      memory: 256Mi
  pprof:
    enabled: true
    uploaderType: pyroscope
    cpuSamplingDuration: 60s
    backoffTicker:
      interval: 10s
      exponent: 40
    memoryTicker:
      enabled: true
      interval: 60s
      jumpSizeInBytes: 100000000
      windowSize: 12h
      initialWait: 10m
    cpuTicker:
      enabled: true
      interval: 60s
      jumpSizeInMCPU: 100
      windowSize: 12h
      initialWait: 10m
  env: []
  builtinEnv:
  - name: API_KEY
    valueFrom:
      secretKeyRef:
        key: '{{ template "groundcover.apikeySecretKey" . }}'
        name: '{{ template "groundcover.apikeySecretName" . }}'        
  readinessProbe:
    httpGet:
      scheme: HTTP
      path: /health
      port: watcher-http
    initialDelaySeconds: 45
    periodSeconds: 15
    failureThreshold: 20
  livenessProbe:
    httpGet:
      scheme: HTTP
      path: /health
      port: watcher-http
    initialDelaySeconds: 45
    periodSeconds: 15
    failureThreshold: 20

journalScraper:
  serviceNames: ["kubelet", "kernel"]
  historyDuration: 10m
  journalFilter: "system.journal"
  includeRemoteJournals: true
  checkInterval: 5s
logFileTargets: []
# - path: /var/log/syslog.*
#   excludedPath: "*.gz"
#   labels:
#     label1: "value1"
#   workload: "syslog" # optional

portal:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  env:
    - name: GC_CLICKHOUSE_HOST
      value: '{{ include "clickhouse.shard0Name" . }}'
  extraHeaders:
  # - Key: "Header Key"
  #   Value: "Header Value"
  image:
    repository: "{{ .Values.global.origin.registry }}/portal"
    tag: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 50m
      memory: 256Mi
  pprof:
    enabled: true
    uploaderType: pyroscope
    cpuSamplingDuration: 60s
    backoffTicker:
      interval: 10s
      exponent: 40
    memoryTicker:
      enabled: true
      interval: 60s
      jumpSizeInBytes: 100000000
      windowSize: 12h
      initialWait: 10m
    cpuTicker:
      enabled: true
      interval: 60s
      jumpSizeInMCPU: 100
      windowSize: 12h
      initialWait: 10m
  kong:
    endpointOverride:
    enabled:
  autodiscovery: false
  maxResponseSizeBytes: 33554432

clickhouse:
  enabled: true
  shards: 1
  replicaCount: 1
  image:
    tag: 24.11.2-debian-12-r0
    registry: public.ecr.aws/groundcovercom
  coldVolume: ""
  containerSecurityContext:
    enabled: false
  podLabels:
    app.kubernetes.io/part-of: '{{ include "groundcover.labels.partOf" . }}'
  podSecurityContext:
    enabled: false
  volumePermissions:
    enabled: true
    image:
      registry: public.ecr.aws/groundcovercom
      repository: bitnami/os-shell
      tag: 11-debian-11-r91
  keeper:
    enabled: true
  zookeeper:
    enabled: false
  externalAccess:
    enabled: true
    service:
      type: ClusterIP
      ports:
        http: 8123
  metrics:
    enabled: true
  allowSimdjson: true
  maxConnections: 10000
  maxDistributedConnections: 10000
  disableOpentelemetrySpanLog: true
  idleConnectionTimeout: 600
  auth:
    existingSecret: "false" # override global.clickhouse.auth.existingSecret to use exisiting secret
  extraEnvVars:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  extraOverrides: |
    <clickhouse>
      {{ include "clickhouse.opentelemetrySpanLogSetting" . }}
      <logger>
          <formatting><type>json</type></formatting>
      </logger>
      <max_connections>{{ .Values.maxConnections }}</max_connections>
      <max_table_size_to_drop>0</max_table_size_to_drop>
      <max_partition_size_to_drop>0</max_partition_size_to_drop>
      <merge_tree>
        <materialize_ttl_recalculate_only>0</materialize_ttl_recalculate_only>
        <max_suspicious_broken_parts_bytes>5368709120</max_suspicious_broken_parts_bytes>
        <max_suspicious_broken_parts>1000</max_suspicious_broken_parts>
      </merge_tree>
      <profiles>
        <default>
          <materialize_ttl_after_modify>0</materialize_ttl_after_modify>
          <idle_connection_timeout>{{ .Values.idleConnectionTimeout }}</idle_connection_timeout>
          <distributed_connections_pool_size>{{ .Values.maxDistributedConnections }}</distributed_connections_pool_size>
          <max_distributed_connections>{{ .Values.maxDistributedConnections }}</max_distributed_connections>                
          <allow_simdjson>{{ .Values.allowSimdjson }}</allow_simdjson>
          <allow_experimental_analyzer>0</allow_experimental_analyzer>
          <date_time_input_format>best_effort</date_time_input_format>
          <async_insert>true</async_insert>
          <max_execution_time>60</max_execution_time>
          <wait_for_async_insert>true</wait_for_async_insert>
          <enable_json_type>1</enable_json_type>
        </default>
      </profiles>
      <query_log>
        <ttl>event_date + INTERVAL 10 DAY</ttl>
      </query_log>
      <trace_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </trace_log>
      <metric_log>
        <ttl>event_date + INTERVAL 3 DAY</ttl>
      </metric_log>
      <processors_profile_log>
        <ttl>event_date + INTERVAL 1 DAY</ttl>
      </processors_profile_log>
      <part_log>
        <ttl>event_date + INTERVAL 1 DAY</ttl>
      </part_log>
      <query_views_log>
        <ttl>event_date + INTERVAL 1 DAY</ttl>
      </query_views_log>
      <text_log>
        <ttl>event_date + INTERVAL 2 DAY</ttl>
      </text_log>
    </clickhouse>
  usersExtraOverrides: |
    <clickhouse>
      <profiles>
        <reader_profile>
          <readonly>0</readonly>
        </reader_profile>
        <writer_profile>
          <readonly>0</readonly>
        </writer_profile>
      </profiles>
      <users>
        <default>
          <default_database>groundcover</default_database>
        </default>
        <manager>
          <default_database>default</default_database>
          <password from_env="CLICKHOUSE_PASSWORD" />
          <networks>
            <ip>::/0</ip>
          </networks>
        </manager>
        <reader>
          <password from_env="CLICKHOUSE_PASSWORD" />
          <networks>
            <ip>::/0</ip>
          </networks>
          <profile>reader_profile</profile>
          <quota>default</quota>
        </reader>
        <writer>
          <password from_env="CLICKHOUSE_PASSWORD" />
          <networks>
            <ip>::/0</ip>
          </networks>
          <profile>writer_profile</profile>
          <quota>default</quota>
        </writer>
      </users>
    </clickhouse>
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.labels Persistent Volume Claim labels
    ##
    labels:
      app.kubernetes.io/managed-by: Helm
      helm.sh/chart: clickhouse-3.2.1
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 256Gi

    # Should STS be removed to allow planned rollout of immutable spec. Incurs downtime.
    dropBeforeCreate: false

  ## ClickHouse resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ## @param resources.limits The resources limits for the ClickHouse containers
  ## @param resources.requests The requested resources for the ClickHouse containers
  ##
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      memory: 8Gi

  ## @param affinity Affinity for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `podAffinityPreset`, `podAntiAffinityPreset`, and `nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param updateStrategy.type ClickHouse statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##

  extraVolumes:
    - name: clickhouse-client-config
      configMap:
        name: clickhouse-client-config-map

  extraVolumeMounts:
    - name: clickhouse-client-config
      mountPath: /etc/clickhouse-client/

opentelemetry-collector:
  enabled: true
  mode: deployment
  command:
    name: otelcol-contrib
  extraEnvs:
    - name: GC_GROUNDCOVERVERSION
      value: "{{ default .Chart.AppVersion .Values.global.origin.tag }}"
    - name: DB_MANAGER_ADDRESS
      value: '{{ include "db-manager.ready.http.url" .}}'
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
    - name: CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: '{{ include "groundcover.config.secretName" .}}'
          key: GC_CLUSTER_ID
  extraVolumes:
    - name: persistent-queues
      emptyDir: {}
  extraVolumeMounts:
    - name: persistent-queues
      mountPath: /persistent_queues
  image:
    repository: public.ecr.aws/groundcovercom/otel/opentelemetry-collector-contrib
    tag: ""
  podLabels:
    app.kubernetes.io/part-of: '{{ include "groundcover.labels.partOf" . }}'
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: '{{ index .Values "ports" "metrics" "containerPort" | int }}'
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: opentelemetry-collector
  ports:
    metrics:
      enabled: true
    pprof:
      enabled: true
      containerPort: 1777
      servicePort: 1777
      hostPort: 1777
      protocol: TCP
    faro:
      enabled: true
      containerPort: 12347
      servicePort: 12347
      hostPort: 12347
      protocol: TCP
    loki-historian:
      enabled: true
      containerPort: 3500
      servicePort: 3500
      hostPort: 3500
      protocol: TCP
    loki-http:
      enabled: true
      containerPort: 3100
      servicePort: 3100
      hostPort: 3100
      protocol: TCP
    health:
      enabled: true
      containerPort: 13133
      servicePort: 13133
      hostPort: 13133
      protocol: TCP
    datadogapm:
      enabled: true
      containerPort: 8126
      servicePort: 8126
      hostPort: 8126
      protocol: TCP
    awsfirehose:
      enabled: true
      containerPort: 4433
      servicePort: 4433
      hostPort: 4433
      protocol: TCP
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      hostPort: 9411
      protocol: TCP
    http-json:
      enabled: true
      containerPort: 8016
      servicePort: 8016
      hostPort: 8016
      protocol: TCP
    rum:
      enabled: true
      containerPort: 8026
      servicePort: 8026
      hostPort: 8026
      protocol: TCP
  config:
    extensions:
      health_check:
        path: /health
        tls: ${env:TLS_CONFIG}
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "health" "containerPort" | int ) }}'
      pprof:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "pprof" "containerPort" | int ) }}'
    receivers:
      http:
        endpoint: 0.0.0.0:8016
      rum/logs:
        endpoint: 0.0.0.0:8026
        rum_data_type: logs
      rum/traces:
        endpoint: 0.0.0.0:8026
        rum_data_type: traces
      rum/events:
        endpoint: 0.0.0.0:8026
        rum_data_type: events
      otlp/dummy_receiver:
        protocols:
          http:
            endpoint: "localhost:9999"
      loki/historian:
        protocols:
          http:
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "loki-historian" "containerPort" | int ) }}'
        use_incoming_timestamp: true
      loki:
        protocols:
          http:
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "loki-http" "containerPort" | int ) }}'
        use_incoming_timestamp: true
      otlp:
        protocols:
          grpc:
            max_recv_msg_size_mib: 50
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "otlp" "containerPort" | int ) }}'
          http:
            tls: ${env:TLS_CONFIG}
            include_metadata: true
            cors:
              allowed_origins:
                - "*"
              allowed_headers:
                - "*"
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "otlp-http" "containerPort" | int ) }}'
      faro:
        http:
          tls: ${env:TLS_CONFIG}
          cors:
            allowed_origins:
              - "*"
            allowed_headers:
              - "*"
          endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "faro" "containerPort" | int ) }}'
      datadog:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "datadogapm" "containerPort" | int ) }}'
      awsfirehose:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "awsfirehose" "containerPort" | int ) }}'
        record_type: "cwlogs"
      zipkin:
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "zipkin" "containerPort" | int ) }}'
    processors:
      batch/monitorState:
        timeout: 5s
        send_batch_size: 1000
      batch/logs:
        timeout: 1s
        send_batch_size: 20000
      batch/custom:
        timeout: 5s
        send_batch_size: 1000
      batch/traces:
        timeout: 1s
        send_batch_size: 5000
      resource/enrich_headers:
        attributes:
        - key: env_name
          from_context: x-groundcover-env-name
          action: upsert
        - key: service.name
          from_context: x-groundcover-service-name
          action: upsert
        - key: gc_env_type
          from_context: x-groundcover-env-type
          action: upsert
      groundcover/logs:
        type: logs
      groundcover/custom:
        type: custom
      groundcover/monitorState:
        type: monitorState
      groundcover/traces: {}
    exporters:
      prometheusremotewrite:
        endpoint: '{{ include "metrics-ingester.cluster.http.write.url" . }}'
        tls:
          insecure: true
        resource_to_telemetry_conversion:
          enabled: true
      otlphttp/vector_logs:
        endpoint: '{{ include "vector.cluster.otlp.http.logs.endpoint" . }}'
        tls:
          insecure: true
      otlphttp/vector_traces:
        endpoint: '{{ include "vector.cluster.otlp.http.traces.endpoint" . }}'
        tls:
          insecure: true
      otlphttp/vector_custom:
        endpoint: '{{ include "vector.cluster.otlp.http.custom.endpoint" . }}'
        tls:
          insecure: true
      otlphttp/vector_monitors:
        endpoint: '{{ include "vector.cluster.otlp.http.monitors.endpoint" . }}'
        tls:
          insecure: true
    connectors:
      groundcover/traces2logs: {}
    service:
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: '0.0.0.0'
                    port: 8888
      extensions: [health_check, pprof]
      pipelines:
        logs/monitorState:
          receivers:
            - loki/historian
          processors:
            - batch/monitorState
            - groundcover/monitorState
          exporters:
            - otlphttp/vector_monitors
        metrics:
          receivers:
            - otlp
          exporters:
            - prometheusremotewrite
        traces:
          receivers:
            - otlp
            - datadog
            - faro
            - rum/traces
            - zipkin
          processors:
            - batch/traces
            - groundcover/traces
          exporters:
            -  groundcover/traces2logs
        logs:
          receivers:
            - loki
            - otlp
            - faro
            - awsfirehose
            - http
            - rum/logs
          processors:
            - resource/enrich_headers
            - batch/logs
            - groundcover/logs
          exporters:
            - otlphttp/vector_logs
        logs/traces:
          receivers:
            - groundcover/traces2logs
          exporters:
            - otlphttp/vector_traces
        logs/custom:
          receivers:
            - otlp
            - rum/events
          processors:
            - batch/custom
            - groundcover/custom
          exporters:
            - otlphttp/vector_custom
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      memory: 2048Mi
  podDisruptionBudget:
    enabled: true
    maxUnavailable: "1%"
  rollout:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
    strategy: RollingUpdate
  affinity: {}
  readinessProbe:
   initialDelaySeconds: 8
  nodeSelector: {}
  tolerations: []

vector:
  rollWorkload: false
  logsPipeline:
    inputs:
      - logs_from_logs
      - json_logs
    defaultSteps:
    - name: dropOldLogs
      transform:
        type: "filter"
        condition: |
          to_unix_timestamp(now()) - 12*3600 < to_unix_timestamp(parse_timestamp!(.timestamp, "%+"))
    extraSteps: []
  # List of steps to perform as part of the pipeline. Will be executed sequentially. Examples:
  # - name: renameAttribute
  #   transform:
  #     type: remap
  #     source: |-
  #       .string_attributes.newName = del(.string_attributes.oldName)
  # - name: parseRegex
  #   transform:
  #     type: remap
  #     source: |-
  #       . |= parse_regex(.content, r'pattern')
  tracesPipeline:
    inputs:
      - traces_from_logs
      - json_traces
    defaultSteps: []
    extraSteps: []
  # List of steps to perform as part of the pipeline. Will be executed sequentially. Examples:
  # - name: renameAttribute
  #   transform:
  #     type: remap
  #     source: |-
  #       .string_attributes.newName = del(.string_attributes.oldName)
  # - name: parseRegex
  #   transform:
  #     type: remap
  #     source: |-
  #       . |= parse_regex(.content, r'pattern')
  eventsPipelines: {}
  # map of pipeline names to pipeline configurations
  # every pipeline configuration is made of a list of steps. for example:
    # random_number_event:
    #   inputs:
    #     - logs_from_logs
    #     - json_logs
    #   defaultSteps: []
    #   extraSteps:
    #   - name: filterWorkloadAndContent
    #     transform:
    #       type: "filter"
    #       condition: |
    #         .workload == "random-logger" && contains(string!(.content), "Random number")
    #   - name: parseRandomNumber
    #     transform:
    #       type: "remap"
    #       source: |-
    #         res = parse_regex!(string!(.content), r'Random number: (?P<number>[\d]+)')
    #         .float_attributes = object!(.float_attributes)
    #         .float_attributes.random_number = to_int!(res.number)
  image:
    repository: public.ecr.aws/groundcovercom/vector
    tag: 0.42.0-alpine
  lifecycle: 
    preStop:
      exec:
        command:
        - /bin/sleep
        - "3"    
  service:
    ports:
      - name: api
        port: 8686
      - name: grpc-logs
        port: 4317
      - name: http-logs
        port: 4318
      - name: json-logs
        port: 4319
      - name: grpc-traces
        port: 4327
      - name: http-traces
        port: 4328
      - name: json-traces
        port: 4329
      - name: grpc-custom
        port: 4337
      - name: http-custom
        port: 4338
      - name: json-events
        port: 4359
      - name: json-entities
        port: 4369
      - name: json-measurements
        port: 4379
      - name: grpc-monitors
        port: 4347
      - name: http-monitors
        port: 4348
      - name: json-monitors
        port: 4349
      - name: metrics
        port: 9598
      - name: vm-metrics
        port: 4599
      - name: statsd-udp
        port: 8125
        protocol: UDP
      - name: statsd-tcp
        port: 8125
      - name: json-table-write
        port: 4350
  containerPorts:
    - name: api
      containerPort: 8686
    - name: grpc-logs
      containerPort: 4317
    - name: http-logs
      containerPort: 4318
    - name: json-logs
      containerPort: 4319
    - name: grpc-traces
      containerPort: 4327
    - name: http-traces
      containerPort: 4328
    - name: json-traces
      containerPort: 4329
    - name: grpc-custom
      containerPort: 4337
    - name: http-custom
      containerPort: 4338
    - name: json-custom
      containerPort: 4339
    - name: grpc-monitors
      containerPort: 4347
    - name: http-monitors
      containerPort: 4348
    - name: json-monitors
      containerPort: 4349
    - name: metrics
      containerPort: 9598
    - name: vm-metrics
      containerPort: 4599
    - name: statsd-udp
      containerPort: 8125
      protocol: UDP
    - name: statsd-tcp
      containerPort: 8125
    - name: json-tablewrite
      containerPort: 4350
  role: "Stateless-Aggregator"
  replicas: 2
  resources:
    requests:
      cpu: 100m
      memory: 2048Mi
    limits:
      memory: 4096Mi
  # backendEnv will only be created if backend is enabled
  backendEnv:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  env:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
    - name: VECTOR_WATCH_CONFIG
      value: "true"
    - name: VECTOR_LOG_FORMAT
      value: "json"
  podAnnotations:
    prometheus.io/port: "9598"
    prometheus.io/scrape: "true"
  podLabels:
    app.kubernetes.io/part-of: groundcover
    app.groundcover.com/component: vector
  existingConfigMaps:
    - vector-config-map
  dataDir: "/data/vector.yaml"
  readinessProbe:
    httpGet:
      path: /health
      port: 8686
    initialDelaySeconds: 2
    timeoutSeconds: 1
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3
  livenessProbe:
    httpGet:
      path: /health
      port: 8686
    initialDelaySeconds: 2
    timeoutSeconds: 1
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  objectStorage:
    allowed: true
    backups: false
    s3Bucket:
    region:
  customGlobalConfig:
    api:
      enabled: true
      playground: false
      graphql: false
      address: "0.0.0.0:8686"
  customComponents:
    sources:
      overrideSources:
      extraSources:
      empty:
        emptySource:
          type: file
          include:
            - /non_exiding_dir/non_existing_file
          exclude:
            - /*
      metrics:
        statsd_udp:
          type: statsd
          address: 0.0.0.0:8125
          mode: udp
        statsd_tcp:
          type: statsd
          address: 0.0.0.0:8125
          mode: tcp
        vm_metrics:
          type: prometheus_remote_write
          address: 0.0.0.0:4599
          mode: tcp
      otel:
        otel_logs:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4317
          http:
            address: 0.0.0.0:4318
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
        otel_logs_custom:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4337
          http:
            address: 0.0.0.0:4338
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
        otel_logs_monitors:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4347
          http:
            address: 0.0.0.0:4348
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
        otel_traces:
          type: opentelemetry
          grpc:
            address: 0.0.0.0:4327
          http:
            address: 0.0.0.0:4328
            keepalive:
              max_connection_age_jitter_factor: 0.1
              max_connection_age_secs: 300
      json:
        json_logs:
          type: http
          address: "0.0.0.0:4319"
          decoding:
            codec: json
          path: ""
          strict_path: false
        json_events:
          type: http
          address: "0.0.0.0:4359"
          decoding:
            codec: json
          path: ""
          strict_path: false
        json_entities:
          type: http
          address: "0.0.0.0:4369"
          decoding:
            codec: json
          path: ""
          strict_path: false
        json_measurements:
          type: http
          address: "0.0.0.0:4379"
          decoding:
            codec: json
          path: ""
          strict_path: false
        json_monitors:
          type: http
          address: "0.0.0.0:4349"
          decoding:
            codec: json
          path: ""
          strict_path: false
        json_traces:
          type: http
          address: "0.0.0.0:4329"
          decoding:
            codec: json
          path: ""
          strict_path: false
        json_table_write:
          type: http
          address: "0.0.0.0:4350"
          path: '{{ include "vector.cluster.json.tables.write.path" . }}'
          strict_path: false
          method: "POST"
          decoding:
            codec: json
    transforms:
      overrideTransforms:
      extraTransforms:
      default:
        logs_from_logs:
          inputs:
            - "otel_logs.logs"
          type: "remap"
          source: |-
            .attributes, err = object(.attributes)
            if err == null {
              if exists(.message) {
                message, err = string(.message)
                if err == null && strlen(message) > 0 {
                  .attributes.body = message
                }
              }

              if exists(.resources) {
                .resources, err = object(.resources)
                if err == null {
                  .attributes = merge(.attributes, .resources, true)
                }
              }

              . = .attributes
            } else {
              log("Recieved non-object attributes, skipping", level: "error")
            }
        traces_from_logs:
          inputs:
            - "otel_traces.logs"
          type: "remap"
          source: |-
            . = .attributes
        custom_from_logs:
          inputs:
            - "otel_logs_custom.logs"
          type: "remap"
          source: |-
            . = .attributes
        custom_from_logs_routing:
          inputs:
            - custom_from_logs
          type: "route"
          reroute_unmatched: true
          route:
            entities: .log_type == "entity"
            measurements: .log_type == "measurement"
        entities:
          inputs:
            - custom_from_logs_routing.entities
          type: "remap"
          source: |-

        table_write_routing:
          inputs:
            - "json_table_write"
          type: "route"
          reroute_unmatched: false
          route:
            metrics_metadata: .path == {{ include "vector.json.tables.write.metrics_metadata.path" . | quote}}
            aws_billing_report: .path == {{ include "vector.json.tables.write.aws_billing_report.path" . | quote}}
        measurements:
          inputs:
            - custom_from_logs_routing.measurements
          type: "remap"
          source: |-

        events:
          inputs:
            - custom_from_logs_routing._unmatched
          type: "remap"
          source: |-
            .type = del(.event_type)
        monitors:
          inputs:
            - "otel_logs_monitors.logs"
          type: "remap"
          source: |-
            . = .attributes
            del(.condition)
            del(.dashboardUID)
            del(.folderUID)
            del(.grafana_folder)
            del(.from)
            del(.group)
            del(.orgID)
            del(.panelID)
            del(.ruleUID)
            .evaluation_error = .error
            del(.error)
            .evaluation_duration_seconds = .evaluationDurationSeconds
            del(.evaluationDurationSeconds)
            .previous_state, err = to_string(del(.previous))
            if err == null {
              .previous_state = to_string(split(.previous_state, "(", 2)[0])
              .previous_state = strip_whitespace(.previous_state)
            }
            .state, err = to_string(del(.current))
            if err == null {
              .state = to_string(split(.state, "(", 2)[0])
              .state = strip_whitespace(.state)
            }
            .cluster = get!(value: .labels, path: ["cluster"])
            if .cluster == "" {
              .cluster = get!(value: .labels, path: ["clusterId"])
            }
            .env = get!(value: .labels, path: ["env"])
            .namespace = get!(value: .labels, path: ["namespace"])
            .workload = get!(value: .labels, path: ["workload"])
            if .workload == "" {
              .workload = get!(value: .labels, path: ["workload_name"])
            } 
            .pod = get!(value: .labels, path: ["pod"])
            if .pod == "" {
              .pod = get!(value: .labels, path: ["pod_name"])
            }
            .category = get!(value: .labels, path: ["category"])
            .severity = get!(value: .labels, path: ["_gc_severity"])
            .measurement_type = get!(value: .annotations, path: ["_gc_measurement_type"])
            del(.annotations)
        metrics_metadata_to_logs:
          inputs:
            - "vm_metrics"
          type: "metric_to_log"
        throttled_metrics_metadata_to_logs:
          inputs:
            - "metrics_metadata_to_logs"
          type: "throttle"
          threshold: 1000
          window_secs: 1
        metrics_metadata_extracted:
          inputs:
            - "throttled_metrics_metadata_to_logs"
          type: "remap"
          source: |-
            . =  {
              "timestamp": .timestamp,
              "cluster": .tags.clusterId,
              "env": .tags.env,
              "namespace": .tags.namespace,
              "name": .name,
              "tags": .tags
            }
      logs_to_events: # inputs list should be injected
        type: "remap"
        source: |-
          if !exists(.event_type) {
            abort "no event_type for custom event"
          }
          . = {
            "event_id": .guid,
            "first_timestamp": .timestamp,
            "last_timestamp": .timestamp,
            "created_timestamp": .timestamp,
            "seen_timestamp": .timestamp,
            "timestamp": .timestamp,
            "env": .env,
            "source": .source,
            "is_external": false,
            "entity_uid": .pod_uid,
            "entity_name": .pod_name,
            "entity_kind": "pod",
            "entity_namespace": .namespace,
            "entity_workload": .workload,
            "category": "custom",
            "type": .event_type,
            "severity": "",
            "k8s_event_uid": "",
            "k8s_reason": "",
            "reason": "",
            "k8s_message": "",
            "message": .content,
            "count": 1,
            "raw": .content,
            "string_attributes": .string_attributes,
            "float_attributes": .float_attributes,
            "cluster": .cluster,
            "env_name": .env,
          }

    sinks:
      overrideSinks:
      extraSinks:
      metrics:
        custom_metrics_sink:
          inputs:
            - statsd_udp
            - statsd_tcp
          type: prometheus_remote_write
          endpoint: '{{ include "metrics-ingester.cluster.http.write.url" . }}'
          healthcheck:
            enabled: false
      remote:
        logs:
          json_logs_sink:
            type: http
            uri: '{{ include "vector.incloud.json.logs.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 50000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
        traces:
          json_traces_sink:
            type: http
            uri: '{{ include "vector.incloud.json.traces-as-logs.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
        custom:
          json_events_sink:
            type: http
            inputs:
            - events
            - json_events
            - logs_to_events_transform
            uri: '{{ include "vector.incloud.json.events.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
          json_entities_sink:
            type: http
            inputs:
              - entities
              - json_entities
            uri: '{{ include "vector.incloud.json.entities.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
          json_measurements_sink:
            type: http
            inputs:
              - measurements
              - json_measurements
            uri: '{{ include "vector.incloud.json.measurements.url" . }}'
            method: post
            encoding:
              codec: json
              json:
                timestamp_format: rfc3339
            compression: zstd
            request:
              retry_attempts: 5
              headers:
                apikey: $API_KEY
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 2000000
              max_events: 2000
              timeout_secs: 1
      local:
        logs:
          clickhouse_logs:
            type: clickhouse
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "logs_v4"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 50000
                when_full: drop_newest
            batch:
              max_bytes: 200000000
              max_events: 200000
              timeout_secs: 1
        traces:
          clickhouse_traces:
            type: clickhouse
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "traces_v2"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 200000000
              max_events: 10000
              timeout_secs: 1
        custom:
          clickhouse_k8s_entities:
            type: clickhouse
            inputs:
              - entities
              - json_entities
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "entities_v1"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_measurements:
            type: clickhouse
            inputs:
              - measurements
              - json_measurements
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "measurements"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_k8s_events:
            type: clickhouse
            inputs:
            - events
            - json_events
            - logs_to_events_transform
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "events_v2"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_monitors:
            type: clickhouse
            inputs:
              - monitors
              - json_monitors
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "monitor_state_v1"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
          clickhouse_metrics_metadata:
            type: clickhouse
            inputs:
              - metrics_metadata_extracted
              - table_write_routing.metrics_metadata
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "metrics_metadata"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 5000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
            healthcheck:
              enabled: false
          clickhouse_aws_billing_report:
            type: clickhouse
            inputs:
              - table_write_routing.aws_billing_report
            endpoint: '{{ include "clickhouse.shard0HttpEndpoint" . }}'
            database: "groundcover"
            table: "aws_billing_report"
            request:
              retry_attempts: 5
            auth:
              user: '{{ include "clickhouse.username" . }}'
              password: $CLICKHOUSE_PASSWORD
              strategy: "basic"
            format: "json_each_row"
            encoding:
              timestamp_format: rfc3339
            date_time_best_effort: true
            buffer:
                type: memory
                max_events: 50000
                when_full: drop_newest
            batch:
              max_bytes: 100000000
              max_events: 10000
              timeout_secs: 2
            healthcheck:
              enabled: false
      s3:
        logs:
          s3_logs:
            type: "aws_s3"
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/logs_v3/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
        traces:
          s3_traces:
            type: "aws_s3"
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/traces_v2/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 2500
              timeout_secs: 5
            healthcheck:
              enabled: false
        custom:
          s3_entities:
            type: "aws_s3"
            inputs:
              - entities
              - json_entities
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/entities/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
          s3_measurements:
            type: "aws_s3"
            inputs:
              - measurements
              - json_measurements
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/measurements/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
          s3_events:
            type: "aws_s3"
            inputs:
            - events
            - json_events
            - logs_to_events_transform
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/events/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
          s3_monitors:
            type: "aws_s3"
            inputs:
            - monitors
            - json_monitors
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "v2/monitors_v2/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            healthcheck:
              enabled: false
      gcs:
        logs:
          gcs_logs:
            type: gcp_cloud_storage
            key_prefix: v2/logs_v3/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
        traces:
          gcs_traces:
            type: gcp_cloud_storage
            key_prefix: v2/traces_v2/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 2500
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
        custom:
          gcs_entities:
            type: gcp_cloud_storage
            inputs:
              - entities
              - json_entities
            key_prefix: v2/entities/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
          gcs_measurements:
            type: gcp_cloud_storage
            inputs:
              - measurements
              - json_measurements
            key_prefix: v2/measurements/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
          gcs_events:
            type: gcp_cloud_storage
            inputs:
              - events
              - json_events
              - logs_to_events_transform
            key_prefix: v2/events/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
          gcs_monitors:
            type: gcp_cloud_storage
            inputs:
              - monitors
              - json_monitors
            key_prefix: v2/monitors_v2/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 25000000
              max_events: 25000
              timeout_secs: 5
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
      s3Backups:
        logs:
          s3_logs_backups:
            type: "aws_s3"
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "backup/30d/v2/logs_v3/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 250000000
              max_events: 300000
              timeout_secs: 10
            healthcheck:
              enabled: false
            filename_time_format: "%Y/%m/%d/%H/%s"
            storage_class: STANDARD_IA
        traces:
          s3_traces_backups:
            type: "aws_s3"
            bucket: '{{ .Values.vector.objectStorage.s3Bucket }}'
            region: '{{ .Values.vector.objectStorage.region }}'
            key_prefix: "backup/30d/v2/traces_v2/"
            encoding:
              codec: json
              timestamp_format: rfc3339
            batch:
              max_bytes: 250000000
              max_events: 300000
              timeout_secs: 10
            healthcheck:
              enabled: false
            filename_time_format: "%Y/%m/%d/%H/%s"
            storage_class: STANDARD_IA
      gcsBackups:
        logs:
          gcs_logs_backup:
            type: gcp_cloud_storage
            key_prefix: backup/30d/v2/logs_v3/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 250000000
              max_events: 300000
              timeout_secs: 10
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
            filename_time_format: "%Y/%m/%d/%H/%s"
            storage_class: NEARLINE
        traces:
          gcs_traces_backups:
            type: gcp_cloud_storage
            key_prefix: backup/30d/v2/traces_v2/
            compression: zstd
            bucket: '{{ .Values.vector.objectStorage.gcsBucket }}'
            batch:
              max_bytes: 250000000
              max_events: 300000
              timeout_secs: 10
            encoding:
              codec: json
              timestamp_format: rfc3339
            healthcheck:
              enabled: false
            filename_time_format: "%Y/%m/%d/%H/%s"
            storage_class: NEARLINE
router:
  # onprem or cloud
  mode: cloud
  enabled: false
  additionalLabels:
  additionalAnnotations:

metrics-ingester:
  nodeSelector:
  rbac:
    enabled: false
  service:
    enabled: true
  podAnnotations:
    prometheus.io/port: "8429"
    prometheus.io/scrape: "true"
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
  extraArgs:
    remoteWrite.url: '{{ include "ingester.buildRemoteWriteURLTargets" . }}'
    remoteWrite.rateLimit: '{{ include "ingester.buildRemoteWriteRateLimit" . }}'
    remoteWrite.maxDiskUsagePerURL: '{{ include "ingester.buildRemoteWriteMaxDiskUsagePerURL" . }}'
    remoteWrite.disableOnDiskQueue: '{{ include "ingester.buildRemoteWriteDisableOnDiskQueue" . }}'
    remoteWrite.forceVMProto: '{{ include "ingester.buildRemoteWriteForceVMProto" . }}'
    remoteWrite.forcePromProto: '{{ include "ingester.buildRemoteWriteForcePromProto" . }}'
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    remoteWrite.showURL: "true"
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.tlsInsecureSkipVerify: '{{ ternary "true" "false" .Values.global.ingestion.tls_skip_verify }}'
    tlsKeyFile: /etc/ssl/gc-certs/tls.key
    tlsCertFile: /etc/ssl/gc-certs/tls.crt
    tls: "{{ .Values.global.metrics.tls.enabled }}"
    usePromCompatibleNaming: "true"
  env:
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  extraVolumes:
    - name: certificate
      secret:
        optional: true
        secretName: metrics-ingester-certificate
  extraVolumeMounts:
    - name: certificate
      readOnly: true
      mountPath: /etc/ssl/gc-certs
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
  rollout:
    maxUnavailable: 0
    maxSurge: 1
    strategy: RollingUpdate
  config:
    scrape_configs: []
  podLabels: 
    app.groundcover.com/pipeline: metrics
victoria-metrics-agent:
  service:
    enabled: true
  nodeSelector:
  rbac:
    namespaced: true
  resources:
    limits:
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  podAnnotations:
    prometheus.io/port: "8429"
    prometheus.io/scrape: "true"
  # imagePullSecrets:
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  extraArgs:
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    remoteWrite.showURL: "true"
    remoteWrite.maxDiskUsagePerURL: 10GB
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.url: '{{ include "telemetry.metrics.url" . }}'
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID),groundcover_version=$(GC_VERSION)
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: '{{ include "groundcover.config.secretName" .}}'
          key: GC_CLUSTER_ID
    - name: GC_VERSION
      valueFrom:
        secretKeyRef:
          name: '{{ include "groundcover.config.secretName" .}}'
          key: GC_VERSION
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  config:
    scrape_configs:
      - job_name: groundcover
        scrape_interval: 30s
        kubernetes_sd_configs:
          - namespaces:
              own_namespace: true
            role: pod
            selectors:
              - role: pod
                label: app!=sensor
        relabel_configs:
          - action: keep
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: ".*groundcover.*"
          - action: drop
            regex: true
            source_labels:
              - __meta_kubernetes_pod_container_init
          - action: keep_if_equal
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_port
              - __meta_kubernetes_pod_container_port_number
          - action: keep
            regex: true
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scrape
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_scheme
            target_label: __scheme__
          - action: replace
            regex: (.+)
            source_labels:
              - __meta_kubernetes_pod_annotation_prometheus_io_path
            target_label: __metrics_path__
          - action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            source_labels:
              - __address__
              - __meta_kubernetes_pod_annotation_prometheus_io_port
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - action: replace
            source_labels:
              - __meta_kubernetes_namespace
            target_label: kubernetes_namespace
          - action: replace
            source_labels:
              - __meta_kubernetes_pod_name
            target_label: kubernetes_pod_name
          - source_labels:
              - __meta_kubernetes_pod_name
            target_label: instance
        tls_config:
          insecure_skip_verify: true

kube-state-metrics:
  resources:
    requests:
      cpu: 200m
  nodeSelector:
  nameOverride: kube-state-metrics
  customLabels:
    app.groundcover.com/owner: groundcover
  service:
    port: 8080
    annotations:
      prometheus.io/port: "8080"
  collectors:
    # - certificatesigningrequests
    - configmaps
    - cronjobs
    - daemonsets
    - deployments
    # - endpoints
    - horizontalpodautoscalers
    # - ingresses
    - jobs
    # - leases
    # - limitranges
    # - mutatingwebhookconfigurations
    # - namespaces
    # - networkpolicies
    - nodes
    - persistentvolumeclaims
    - persistentvolumes
    # - poddisruptionbudgets
    - pods
    - replicasets
    - replicationcontrollers
    - resourcequotas
    # - secrets
    # - services
    - statefulsets
  # - storageclasses
  # - validatingwebhookconfigurations
  # - volumeattachments

victoria-metrics-operator:
  enabled: false
  # https://github.com/VictoriaMetrics/operator/blob/master/docs/vars.md
  env:
    - name: VM_DISABLESELFSERVICESCRAPECREATION
      value: "true"
    - name: VM_APPREADYTIMEOUT
      value: "900s"
    - name: VM_PODWAITREADYTIMEOUT
      value: "900s"
    - name: VM_USECUSTOMCONFIGRELOADER # https://docs.victoriametrics.com/operator/resources/readme/#configuration-synchronization
      value: "true"
  builtinVMAgent:
    enabled: false
    spec:
      podScrapeSelector: {}
      serviceScrapeSelector: {}
      nodeScrapeSelector: {}
      staticScrapeSelector: {}
      probeSelector: {}
      replicaCount: 1
      remoteWrite:
        - url: '{{ include "custom-metrics.write.http.url" . }}'

custom-metrics:
  enabled: false
  useSensorAsEndpoint: false
  service:
    enabled: true
  nodeSelector:
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8429"
  rbac:
    namespaced: false
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  resources:
    limits:
      memory: 1024Mi
    requests:
      cpu: 100m
      memory: 256Mi
  extraArgs:
    remoteWrite.url: '{{ include "ingester.buildRemoteWriteURLTargets" . }}'
    remoteWrite.rateLimit: '{{ include "ingester.buildRemoteWriteRateLimit" . }}'
    remoteWrite.maxDiskUsagePerURL: '{{ include "ingester.buildRemoteWriteMaxDiskUsagePerURL" . }}'
    remoteWrite.disableOnDiskQueue: '{{ include "ingester.buildRemoteWriteDisableOnDiskQueue" . }}'
    remoteWrite.forceVMProto: '{{ include "ingester.buildRemoteWriteForceVMProto" . }}'
    remoteWrite.forcePromProto: '{{ include "ingester.buildRemoteWriteForcePromProto" . }}'
    remoteWrite.showURL: "true"
    remoteWrite.maxDailySeries:  '{{ ternary "1000000" "5000000" (empty .Values.global.ingress.site) }}'
    remoteWrite.maxHourlySeries: '{{ ternary "200000" "1000000" (empty .Values.global.ingress.site) }}'
    remoteWrite.headers: "apikey:%{API_KEY}"
    remoteWrite.tlsInsecureSkipVerify: '{{ ternary "true" "false" .Values.global.ingestion.tls_skip_verify }}'
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID),cluster=$(GC_CLUSTER_ID),env=$(GC_ENV),env_type=k8s
    promscrape.streamParse: "true"
    promscrape.dropOriginalLabels: "true"
    promscrape.suppressDuplicateScrapeTargetErrors: "true"
    promscrape.maxScrapeSize: "64MiB"
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: '{{ include "groundcover.config.secretName" .}}'
          key: GC_CLUSTER_ID
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
    - name: GC_ENV
      valueFrom:
        secretKeyRef:
          name: '{{ include "groundcover.config.secretName" .}}'
          key: GC_ENV
  extraScrapeConfigs: []
  config:
    scrape_configs:
      - job_name: "kubernetes-pods"
        honor_labels: true
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_part_of
            regex: groundcover
          - action: drop
            source_labels:
              - __meta_kubernetes_pod_label_app_kubernetes_io_name
              - __meta_kubernetes_pod_label_app_groundcover_com_owner
            regex: kube-state-metrics;groundcover
          - action: drop
            source_labels: [__meta_kubernetes_pod_container_init]
            regex: true
          - source_labels:
              [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels:
              [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: service
          - source_labels: [__meta_kubernetes_service_name]
            target_label: job
            replacement: ${1}
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

victoria-metrics-single:
  # imagePullSecrets: []
  server:
    image:
      repository: public.ecr.aws/groundcovercom/victoria-metrics
      tag: v1.103.0

    extraArgs:
      search.maxUniqueTimeseries: "2000000"
      search.maxSeries: "30000"
      search.maxTagKeys: "200000"
      search.maxTagValues: "500000"
      search.maxQueryLen: "100000"
      search.maxConcurrentRequests: "8"
      maxLabelsPerTimeseries: "50"
      promscrape.maxScrapeSize: "100MB"
      streamAggr.config: "/extra-config/aggregation_config.yaml"
      selfScrapeInterval: "30s"
      usePromCompatibleNaming: "true"

    # -- Data retention period, {amount}[h(ours), d(ays), w(eeks), y(ears)], default is 1 month
    retentionPeriod: 7d
    # -- Sts/Deploy additional labels
    extraLabels: {}
    # -- Pod's additional labels
    podLabels: 
      app.groundcover.com/pipeline: metrics
    # -- Pod's annotations
    podAnnotations:
      prometheus.io/port: "8428"
      prometheus.io/scrape: "true"
    # -- Name of Priority Class
    priorityClassName:

    service:
      # -- Service annotations
      annotations: {}
      # -- Service labels
      labels: {}
      # -- Service ClusterIP
      clusterIP: None

    matchLabels: {}

    statefulSet:
      annotations: {}
      # -- Headless service labels
      labels: {}

    persistentVolume:
      # enabled: true
      # -- Persistant volume annotations
      annotations: {}

      # -- StorageClass to use for persistent volume. Requires server.persistentVolume.enabled: true. If defined, PVC created automatically
      storageClass:

      # -- Use this to override the prefix for the pvc, the suffix is auto-generated by k8s according to the pod name
      #pvcNameOverride:

      size: 100Gi
    extraVolumes:
      - name: extra-config
        configMap:
          name: victoria-metrics-aggregation-config
    extraVolumeMounts:
      - name: extra-config
        mountPath: "/extra-config"
        readOnly: true

    resources:
      requests:
        cpu: 1000m
        memory: 3000Mi
      limits:
        memory: 3000Mi

    # -- Node tolerations for server scheduling to nodes with taints. Ref: [https://kubernetes.io/docs/concepts/configuration/assign-pod-node/](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)
    tolerations: []

    # -- Pod's node selector. Ref: [https://kubernetes.io/docs/user-guide/node-selection/](https://kubernetes.io/docs/user-guide/node-selection/)
    nodeSelector: {}

    # -- Pod affinity
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: eks.amazonaws.com/capacityType
                  operator: NotIn
                  values:
                    - SPOT

    extraAggregations: []
    
monitors-manager:
  initChownData:
    image:
      tag: "1.36.1"
  image:
    registry: public.ecr.aws
    repository: groundcovercom/grafana-groundcover
    tag: v0.0.35-grafana11.3.7
  nameOverride: monitors-manager
  persistence:
    enabled: true
    type: statefulset
    size: 5Gi
  envValueFrom:
    API_KEY:
      secretKeyRef:
        key: '{{ include "groundcover.apikeySecretKey" . }}'
        name: '{{ include "groundcover.apikeySecretName" . }}'
    CLICKHOUSE_PASSWORD:
      secretKeyRef:
        key: '{{ include "clickhouse.secretKey" . }}'
        name: '{{ include "clickhouse.secretName" . }}'
    GF_DATABASE_PASSWORD:
      secretKeyRef:
        name: '{{ include "postgresql.secretName" . }}'
        key: '{{ include "postgresql.adminPasswordKey" . }}'
    KEEP_WEB_HOOK_API_KEY:
      secretKeyRef:
        optional: true
        key: webhook-key
        name: keep-credentials
  extraConfigmapMounts:
    - name: alerting-overrides
      mountPath: /etc/grafana/provisioning/alerting/
      configMap: alerting-overrides-config
      readOnly: true
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: VM
          type: prometheus
          uid: victoria-metrics
          url: '{{ include "victoria-metrics.read.http.url" . }}'
          access: proxy
          jsonData:
            prometheusType: "Prometheus"
            prometheusVersion: "2.50.1"
        - name: Clickhouse
          type: grafana-clickhouse-datasource
          uid: clickhouse
          url: '{{ include "clickhouse.shard0Name" . }}:{{ .Values.global.clickhouse.containerPorts.tcp | int }}'
          jsonData:
            defaultDatabase: '{{ include "clickhouse.database" . }}'
            port: "{{ .Values.global.clickhouse.containerPorts.tcp | int }}"
            server: '{{ include "clickhouse.shard0Name" . }}'
            username: '{{ include "clickhouse.username" . }}'
          access: proxy
          secureJsonData:
            password: $CLICKHOUSE_PASSWORD
  alertingOverrides:
      contactPoints:
        - name: workflows-webhook
          receivers:
            - uid: workflows-webhook
              type: webhook
              settings:
                url: '{{ include "keep.event.alert.url" . }}'
                httpMethod: "POST"
                authorization_scheme: digest
                authorization_credentials: $KEEP_WEB_HOOK_API_KEY
      policies:
      - receiver: workflows-webhook
        group_by:
          - '...'
        group_wait: 30s
        group_interval: 1m
        repeat_interval: 4h
  grafana.ini:
    groundcover:
      alerting_overrides_checksum: '{{ .Values.alertingOverrides | quote | sha256sum }}'
      workflows_enabled: '{{ .Values.global.workflows.enabled }}'
    "unified_alerting.state_history":
      enabled: true
      backend: "loki"
      loki_remote_url: '{{ include "opentelemetry-collector.loki-historian.http.url" . }}'
      log_all: true
      otel_export_enabled: true
      otel_endpoint: '{{ include "ingestion.monitors.otlp.http.url" . }}'
      otel_enable_tls: false
      otel_tls_skip_verify: true
      otel_write_timeout: 10s
    feature_toggles:
      enable: "alertStateHistoryLokiSecondary, alertStateHistoryLokiPrimary, alertStateHistoryLokiOnly, disableInstanceStore"
    server:
      enable_gzip: true
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /usr/share/grafana/plugins
      provisioning: /etc/grafana/provisioning
    database:
      type: postgres
      host: '{{ (include "postgresql.base.url" .) }}'
      name: monitors_manager
      user: postgres
      ssl_mode: require
  extraInitContainers:
    - name: wait-for-db
      image: '{{ include "postgresql.image" . }}'
      env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: '{{ include "postgresql.secretName" . }}'
              key: '{{ include "postgresql.adminPasswordKey" . }}'
      command:
        - /bin/sh
        - -c
      args:
        - |
          pg_isready \
            -U "postgres" \
            -d "dbname=postgres" \
            -h {{ splitList ":" (include "postgresql.base.url" .) | first }} \
            -p {{ splitList ":" (include "postgresql.base.url" .) | last }}
  command:
    - "sh"
    - "-c"
    - "grafana cli --homepath=\"${GF_PATHS_HOME}\" --config=\"${GF_PATHS_CONFIG}\" admin reset-admin-password \"${GF_SECURITY_ADMIN_PASSWORD}\" && /run.sh"

curl:
  image:
    repository: '{{ .Values.global.origin.registry }}/curl' 
    tag: 8.6.0

rbac:
  pspEnabled: false
  sccEnabled: true
  labels:
  annotations:

apikey:
  labels:
  annotations:

config:
  labels:
  annotations:

backend:
  postgresql:
    dbs:
      monitors-manager: monitors_manager
      keep: keep
  keep:
    backend:
      resources:
        limits:
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi

groundcover:
  extraObjects: {}
  podDisruptionBudget:
