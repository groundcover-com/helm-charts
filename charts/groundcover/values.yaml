global:
  groundcover_token:
  # groundcover_token is preceding groundcoverPredefinedTokenSecret, make sure its empty if using existing secret
  # if the secret is preloaded in the namespace, you can refer it here instead
  # example for a preloaded secret :
  # apiVersion: v1
  # kind: Secret
  # metadata:
  #   name: <secretName>
  # stringData:
  #   <secretKey>: <apikey>
  # type: Opaque
  groundcoverPredefinedTokenSecret:
    # the name of the secret
    secretName:
    # the key in the secret containing the token value
    secretKey:
  origin:
    registry: public.ecr.aws/groundcovercom
    tag: "" # rewrites Chart.AppVersion
  imagePullSecrets: []
  groundcoverLabels:
  clickhouse:
    auth:
      # override to use exisiting secret
      existingSecret: ""
      existingSecretKey: admin-password
  logs:
    overrideUrl: ""
    retentionDays: 3
  otlp:
    tls:
      enabled: true
    overrideGrpcURL: ""
    overrideHttpURL: ""
tags:
  # InCloud - Enterprise Setup. groundcover's control-plane reconciled observability infrastructure deployment.
  incloud: false
# optional parameters: stable, legacy, experimental
mode: stable
metrics:
  enabled: true
  host: metrics.groundcover.com
logging:
  enabled: true
  host: logs.groundcover.com
  scheme: https
# SAAS parameters, token is mendatory, others only if in cloud mode
saas:
  tls_skip_verify: false
  scheme: wss
  host: app.groundcover.com
  port: 443
#GENERAL
clusterId:
installationId:
# multipleClusterIds:
#  - clusterA
#  - clusterB
region:
storeIssuesLogsOnly: false
tracesNamespaceFilters: []
# - matchType: "allow"
#   regex: "my-namespace"
tracesWorkloadFilters: []
# - matchType: "block"
#   regex: "do-not-show-me"
logsDropFilters: []
# - '{namespace="demo-ng",workload="loadgenerator"} |~ ".*GET.*"'
shouldDropRunningNamespaces: true
commitHashKeyName:
repositoryUrlKeyName:
priorityClass:
  create: false
  name:
  value:
  preemptionPolicy:
#imagePullSecrets: []
backend:
  enabled: true
agent:
  enabled: true
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  tolerations: []
  affinity:
  nodeSelector:
  priorityClassName:
  hostNetwork: false
  monitoring:
    port: 9102
  alligator:
    image:
      repository: "{{ .Values.global.origin.registry }}/alligator"
    resources:
      requests:
        memory: 300Mi
        cpu: 160m
      limits:
        memory: 700Mi
        cpu: 800m
    obfuscateData: false
    dataRetention: 24h
    nodelabels: []
    contentTypesToDrop: ["text/html", "text/javascript", "text/css", "image", "font", "video", "audio"]
    watchOnlyLocalNode: false
    shepherd:
      handlerFlushInterval: 1m0s
    pprofAlligator:
      enabled: true
      interval: 10s
      exponent: 40
      cpuSamplingDuration: 60s
      pushURL: ""
    env:
  tracy:
    image:
      repository: "{{ .Values.global.origin.registry }}/tracy"
    resources:
      requests:
        memory: 512Mi
        cpu: 125m
      limits:
        memory: 2048Mi
        cpu: 750m
k8sWatcher:
  image:
    repository: "{{ .Values.global.origin.registry }}/k8s-watcher"
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  resources:
    limits:
      cpu: 100m
      memory: 300Mi
    requests:
      cpu: 50m
      memory: 200Mi
  env:
portal:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
  nodeSelector:
  tolerations: []
  priorityClassName:
  extraHeaders:
  # - Key: "Header Key"
  #   Value: "Header Value"
  image:
    repository: "{{ .Values.global.origin.registry }}/portal"
  resources:
    limits:
      cpu: 100m
      memory: 200Mi
    requests:
      cpu: 50m
      memory: 100Mi
clickhouse:
  enabled: true
  shards: 1
  replicaCount: 1
  image:
    registry: public.ecr.aws/groundcovercom
  zookeeper:
    enabled: false
  metrics:
    enabled: true
  auth:
    existingSecret: "false" # override global.clickhouse.auth.existingSecret to use exisiting secret
  extraEnvVars:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
  extraOverrides: "<clickhouse>\n  <profiles>\n    <default>\n      <async_insert>true</async_insert>\n      <wait_for_async_insert>true</wait_for_async_insert>\n    </default>\n  </profiles>\n</clickhouse>          \n"
  startdbScripts:
    init-db.sh: |
      #!/bin/bash
      set -e
      clickhouse client \
        --user "${CLICKHOUSE_ADMIN_USER}" \
        --password "${CLICKHOUSE_ADMIN_PASSWORD}" \
        --database default \
        --query "CREATE DATABASE IF NOT EXISTS {{ include "clickhouse.database" . }};"
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.labels Persistent Volume Claim labels
    ##
    labels: {}
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 128Gi
  ## ClickHouse resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ## @param resources.limits The resources limits for the ClickHouse containers
  ## @param resources.requests The requested resources for the ClickHouse containers
  ##
  resources:
    requests:
      cpu: 300m
      memory: 2048Mi
    limits:
      memory: 3000Mi
  ## @param affinity Affinity for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: `podAffinityPreset`, `podAntiAffinityPreset`, and `nodeAffinityPreset` will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for ClickHouse pods assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param updateStrategy.type ClickHouse statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##

  extraVolumes:
    - name: clickhouse-client-config
      configMap:
        name: clickhouse-client-config-map
  extraVolumeMounts:
    - name: clickhouse-client-config
      mountPath: /etc/clickhouse-client/
opentelemetry-collector:
  enabled: true
  mode: statefulset
  statefulset:
    podManagementPolicy: "OrderedReady"
  extraEnvs:
    - name: CLICKHOUSE_PASSWORD
      valueFrom:
        secretKeyRef:
          key: '{{ include "clickhouse.secretKey" . }}'
          name: '{{ include "clickhouse.secretName" . }}'
    - name: TLS_CONFIG
      value: '{{ include "opentelemetry-collector.tlsConfig" . }}'
  secretMounts:
    - name: certificate
      mountPath: /etc/ssl/certs
      secretName: opentelemetry-collector-certificate
  image:
    repository: public.ecr.aws/groundcovercom/otel/opentelemetry-collector-contrib
    tag: "1.3.5"
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: '{{ index .Values "ports" "metrics" "containerPort" | int }}'
  ports:
    metrics:
      enabled: true
    loki-http:
      enabled: true
      containerPort: 3100
      servicePort: 3100
      hostPort: 3100
      protocol: TCP
    health:
      enabled: true
      containerPort: 13133
      servicePort: 13133
      hostPort: 13133
      protocol: TCP
  config:
    extensions:
      health_check:
        path: /health
        tls: ${env:TLS_CONFIG}
        endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "health" "containerPort" | int ) }}'
    receivers:
      loki:
        protocols:
          http:
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "loki-http" "containerPort" | int ) }}'
        use_incoming_timestamp: true
      otlp:
        protocols:
          grpc:
            tls: ${env:TLS_CONFIG}
            endpoint: '{{ printf "0.0.0.0:%d" (index .Values "ports" "otlp" "containerPort" | int ) }}'
    processors:
      batch/logs:
        timeout: 5s
        send_batch_size: 20000
      batch/k8s:
        timeout: 5s
        send_batch_size: 1000
    exporters:
      groundcover/logs:
        type: logs
        timeout: 5s
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
        ttl_days: '{{ include "logs.retentionDays" . }}'
      groundcover/k8s:
        type: k8s
        timeout: 5s
        ttl_days: 5
        logs_table_name: k8s_objects
        password: ${env:CLICKHOUSE_PASSWORD}
        username: '{{ include "clickhouse.username" . }}'
        database: '{{ include "clickhouse.database" . }}'
        endpoint: '{{ include "clickhouse.nativeEndpoint" . }}'
    service:
      telemetry:
        metrics:
          address: '{{ printf "0.0.0.0:%d" (index .Values "ports" "metrics" "containerPort" | int ) }}'
      pipelines:
        metrics:
          receivers:
            - otlp
        logs:
          receivers:
            - loki
          processors:
            - batch/logs
          exporters:
            - groundcover/logs
        logs/k8s:
          receivers:
            - otlp
          processors:
            - batch/k8s
          exporters:
            - groundcover/k8s
  initContainers:
    - args:
        - while [ $(curl -sw '%{http_code}' {{ printf "%s/ping" (include "clickhouse.httpEndpoint" .) }} -o /dev/null) -ne 200 ]; do echo 'Waiting for Clickhouse...'; sleep 2; done; echo Clickhouse is up
      command:
        - /bin/sh
        - -c
      image: '{{ printf "%s/curl:latest" .Values.global.origin.registry }}'
      name: check-ch-ready
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1024Mi
  affinity: {}
  nodeSelector: {}
  tolerations: []
router:
  # onprem or cloud
  mode: cloud
  enabled: false
  additionalLabels:
  additionalAnnotations:
  repository: router
  origin:
    registry: public.ecr.aws/groundcovercom
# These are the recomendded presets
# if you are after an uninstallation, prior to re-installation make sure
# you've deleted the pvcs:  kubectl delete pvc wal-volume-<release-name>-groundcover-tsdb-0  server-volume-<release-name>-groundcover-tsdb-0
# you've deleted the endpoints:
# kubectl delete svc <release_name>-config
# kubectl delete endpoints <release_name>
# if you installed groundcover in its own namespace, deleting the namespace can done instead
timescaledb-single:
  # imagePullSecrets: []
  image:
    repository: "{{ .Values.global.origin.registry }}/timescaledb-ha"
  tolerations: []
  podAnnotations: {}
  nodeSelector: {}
  #affinity: {}
  # -- Name of Priority Class
  priorityClassName:
  service:
    primary:
      lables: {}
      annotations: {}
      clusterIP:
  # NFS alternative to persistentVolume, mutual exclusive with persistentVolume.data.enabled and persistentVolume.wal.enabled
  # nfs:
  #   enabled: false
  #   server:
  #   path:
  persistentVolumes:
    data:
      # enabled: true
      storageClass:
      size: 100Gi
      annotations: {}
      # -- Use this to override the prefix for the pvc, the suffix is auto-generated by k8s according to the pod name
      #pvcNameOverride:
      # NFS option for the data storage, mutual exclusive with  data.enabled
    wal:
      # enabled: true
      storageClass:
      size: 10Gi
      annotations: {}
      # -- Use this to override the prefix for the pvc, the suffix is auto-generated by k8s according to the pod name
      #pvcNameOverride:
  resources:
    requests:
      cpu: 1000m
      memory: 2000Mi
    limits:
      cpu: 1000m
      memory: 2000Mi
  job:
    tolerations: []
    nodeSelector: {}
    affinity: {}
  # By default the timescaledb secrets are randomly generated, and are re-using the existing values incase of using `helm upgrade`
  # in case of a re-installation (uninstall and install), the wal-* and storage-* pvcs must be deleted as well.
  # if you are interested in upgrading an existing installation using a different toolkit from helm, that uses helm template,
  # you can state the existing secret, in our vanilla installation its name is groundcover-tsdb-certificate
  secrets:
    credentialsSecretName: ""
victoria-metrics-agent:
  rbac:
    namespaced: true
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  # imagePullSecrets:
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  remoteWriteUrls:
    - https://metrics.groundcover.com/insert/prometheus?apikey=$(API_KEY)
  extraArgs:
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID),region=$(GC_REGION),groundcover_version=$(GC_VERSION)
    remoteWrite.maxDiskUsagePerURL: 1GB
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
    - name: GC_REGION
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_REGION
    - name: GC_VERSION
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_VERSION
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  config:
    scrape_configs:
      - job_name: "vmagent"
        stream_parse: true
        static_configs:
          - targets: ["localhost:8429"]
      - job_name: "groundcover"
        stream_parse: true
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              own_namespace: true
        tls_config:
          insecure_skip_verify: true
        relabel_configs:
          - action: drop
            source_labels: [__meta_kubernetes_pod_container_init]
            regex: true
          - action: keep_if_equal
            source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: instance
kube-state-metrics:
  nameOverride: kube-state-metrics
  enabled: false
  service:
    port: 8080
    annotations:
      prometheus.io/port: "8080"
custom-metrics:
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8429"
  enabled: false
  rbac:
    namespaced: false
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  image:
    repository: "{{ .Values.global.origin.registry }}/vmagent"
  service:
    enabled: true
  extraArgs:
    remoteWrite.maxDailySeries: "1000000"
    remoteWrite.maxHourlySeries: "100000"
    remoteWrite.maxDiskUsagePerURL: 1GB
    remoteWrite.label: clusterId=$(GC_CLUSTER_ID)
  env:
    - name: GC_CLUSTER_ID
      valueFrom:
        secretKeyRef:
          name: groundcover-config
          key: GC_CLUSTER_ID
    - name: API_KEY
      valueFrom:
        secretKeyRef:
          key: '{{ include "groundcover.apikeySecretKey" . }}'
          name: '{{ include "groundcover.apikeySecretName" . }}'
  remoteWriteUrls:
    - http://groundcover-victoria-metrics:8428/api/v1/write?apikey=$(API_KEY)
  extraScrapeConfigs:
    ## COPY from Prometheus helm chart https://github.com/helm/charts/blob/master/stable/prometheus/values.yaml

    # Scrape config for API servers.
    #
    # Kubernetes exposes API servers as endpoints to the default/kubernetes
    # service so this uses `endpoints` role and uses relabelling to only keep
    # the endpoints associated with the default/kubernetes service using the
    # default named port `https`. This works for single API server deployments as
    # well as HA API server deployments.
    - job_name: "kubernetes-apiservers"
      kubernetes_sd_configs:
        - role: endpointslices
      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https
      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      # Keep only the default/kubernetes service endpoints for the https port. This
      # will add targets for each API server which Kubernetes adds an endpoint to
      # the default/kubernetes service.
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          action: drop
          regex: groundcover.*
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
    - job_name: "kubernetes-nodes"
      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https
      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          action: drop
          regex: groundcover.*
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/$1/proxy/metrics
    # Scrape config for service endpoints.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
    # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
    # to set this to `https` & most likely set the `tls_config` of the scrape config.
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: If the metrics are exposed on a different port to the
    # service then set this appropriately.
    - job_name: "kubernetes-service-endpoints"
      kubernetes_sd_configs:
        - role: endpointslices
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          action: drop
          regex: groundcover.*
        - action: drop
          source_labels: [__meta_kubernetes_pod_container_init]
          regex: true
        - action: keep_if_equal
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_container_name]
          target_label: container
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: service
        - source_labels: [__meta_kubernetes_service_name]
          target_label: job
          replacement: ${1}
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: node
    # Scrape config for slow service endpoints; same as above, but with a larger
    # timeout and a larger interval
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
    # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
    # to set this to `https` & most likely set the `tls_config` of the scrape config.
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: If the metrics are exposed on a different port to the
    # service then set this appropriately.
    - job_name: "kubernetes-service-endpoints-slow"
      scrape_interval: 5m
      scrape_timeout: 30s
      kubernetes_sd_configs:
        - role: endpointslices
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          action: drop
          regex: groundcover.*
        - action: drop
          source_labels: [__meta_kubernetes_pod_container_init]
          regex: true
        - action: keep_if_equal
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_container_name]
          target_label: container
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: service
        - source_labels: [__meta_kubernetes_service_name]
          target_label: job
          replacement: ${1}
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: node
    # Example scrape config for probing services via the Blackbox Exporter.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/probe`: Only probe services that have a value of `true`
    - job_name: "kubernetes-services"
      metrics_path: /probe
      params:
        module: [http_2xx]
      kubernetes_sd_configs:
        - role: service
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          action: drop
          regex: groundcover.*
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
          action: keep
          regex: true
        - source_labels: [__address__]
          target_label: __param_target
        - target_label: __address__
          replacement: blackbox
        - source_labels: [__param_target]
          target_label: instance
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: service
    # Example scrape config for pods
    #
    # The relabeling allows the actual pod scrape endpoint to be configured via the
    # following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
    - job_name: "kubernetes-pods"
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          action: drop
          regex: groundcover.*
        - action: drop
          source_labels: [__meta_kubernetes_pod_container_init]
          regex: true
        - action: keep_if_equal
          source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_container_name]
          target_label: container
        - source_labels: [__meta_kubernetes_namespace]
          target_label: namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: service
        - source_labels: [__meta_kubernetes_service_name]
          target_label: job
          replacement: ${1}
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: node
  config:
    scrape_configs:
      - job_name: custom-metrics-vmagent
        static_configs:
          - targets: ["localhost:8429"]
      - job_name: "kube-state-metrics"
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              own_namespace: true
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
            action: keep
            regex: kube-state-metrics
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: kubernetes_node
victoria-metrics-single:
  # imagePullSecrets: []
  server:
    image:
      repository: "{{ .Values.global.origin.registry }}/victoria-metrics"
    # -- Data retention period, {amount}[h(ours), d(ays), w(eeks), y(ears)], default is 1 month
    retentionPeriod: 7d
    # -- Sts/Deploy additional labels
    extraLabels: {}
    # -- Pod's additional labels
    podLabels: {}
    # -- Pod's annotations
    podAnnotations: {}
    # -- Name of Priority Class
    priorityClassName:
    service:
      # -- Service annotations
      annotations: {}
      # -- Service labels
      labels: {}
      # -- Service ClusterIP
      clusterIP: None
    matchLabels: {}
    statefulSet:
      annotations: {}
      # -- Headless service labels
      labels: {}
    persistentVolume:
      # enabled: true
      # -- Persistant volume annotations
      annotations: {}
      # -- StorageClass to use for persistent volume. Requires server.persistentVolume.enabled: true. If defined, PVC created automatically
      storageClass:
      # -- Use this to override the prefix for the pvc, the suffix is auto-generated by k8s according to the pod name
      #pvcNameOverride:
      size: 100Gi
    # NFS alternative to persistentVolume, mutual exclusive with persistentVolume.enabled
    # nfs:
    #   enabled: false
    #   server:
    #   path:
    resources:
      requests:
        cpu: 1000m
        memory: 3000Mi
      limits:
        cpu: 1000m
        memory: 3000Mi
    # -- Node tolerations for server scheduling to nodes with taints. Ref: [https://kubernetes.io/docs/concepts/configuration/assign-pod-node/](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)
    tolerations: []
    # -- Pod's node selector. Ref: [https://kubernetes.io/docs/user-guide/node-selection/](https://kubernetes.io/docs/user-guide/node-selection/)
    nodeSelector: {}
    # -- Pod affinity
    affinity: {}
rbac:
  pspEnabled: false
  sccEnabled: true
  labels:
  annotations:
apikey:
  labels:
  annotations:
config:
  labels:
  annotations:
shepherd:
  # Default values for shepherd.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  replicaCount: 1
  image:
    repository: "{{ .Values.global.origin.registry }}/shepherd"
    pullPolicy: Always
    tag:
  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
  podAnnotations: {}
  podSecurityContext: {}
  # fsGroup: 2000

  securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

  service:
    annotations: {}
    type: ClusterIP
    grpcPort: 9991
    httpPort: 8080
  httpIngress:
    enabled: false
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    extraLabels: {}
    hosts: []
    #   - host: shepherd.local
    #     path: /
    #     port: http
    tls: []
    #   - secretName: shepherd-ingress-tls
    #     hosts:
    #       - shepherd.local
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # -- pathType is only for k8s >= 1.1=
    pathType: Prefix
  grpcIngress:
    enabled: false
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    extraLabels: {}
    hosts: []
    #   - host: shepherd.local
    #     path: /
    #     port: http
    tls: []
    #   - secretName: shepherd-ingress-tls
    #     hosts:
    #       - shepherd.local
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # -- pathType is only for k8s >= 1.1=
    pathType: Prefix
  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  nodeSelector: {}
  tolerations: []
  priorityClassName:
  config:
    grpcPort: 9991
    httpPort: 8080
    groundcoverVersion: ""
    requestMaxSizeBytes: 52428800 # 50MB
    timescaledb:
      host: groundcover-tsdb
      port: 5432
      username: postgres
      password: groundcover
      database: groundcover
      sslMode: disable
    ingestor:
      resourceLabels: ["resource_id"]
      entityLabels: ["entity_id", "type"]
      defaultMaxTopResources: 40
      maxTopResources:
        - labels:
            type: "http"
          value: 40
        - labels:
            type: "grpc"
          value: 40
        - labels:
            type: "postgresql"
          value: 40
        - labels:
            type: "mysql"
          value: 40
        - labels:
            type: "redis"
          value: 40
        - labels:
            type: "kafka"
          value: 40
        - labels:
            type: "container_info"
          value: 40
        - labels:
            type: "container_state"
          value: 40
        - labels:
            type: "dns"
          value: 40
        - labels:
            type: "container_restart"
          value: -1
        - labels:
            type: "unavailable_deployment"
          value: -1
      activeDuration: 1m
      metricTTL: 5m
      TLSCertFile: "/var/groundcover/server/tls.crt"
      TLSKeyFile: "/var/groundcover/server/tls.key"
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - victoria-metrics
        - weight: 50
          podAffinityTerm:
            topologyKey: kubernetes.io/zone
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - victoria-metrics
dbManager:
  additionalLabels:
  podLabels:
  additionalAnnotations:
  podAnnotations:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - timescaledb
        - weight: 50
          podAffinityTerm:
            topologyKey: kubernetes.io/zone
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - timescaledb
  nodeSelector:
  tolerations: []
  priorityClassName:
  image:
    repository: "{{ .Values.global.origin.registry }}/db-manager"
  resources:
    limits:
      cpu: 45m
      memory: 45Mi
    requests:
      cpu: 15m
      memory: 15Mi
  dataRetention: "{{ .Values.agent.alligator.dataRetention }}"
